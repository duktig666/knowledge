# MySQL面经面试题

MySQL核心面试题可参考：[字节跳动技术总监整理的这份MySQL学习文档，看完才发现要学的可太多了！](https://zhuanlan.zhihu.com/p/352066490)

## 1. 索引

参看：

- [深入理解 Mysql 索引底层原理](https://zhuanlan.zhihu.com/p/113917726) （mysql索引的底层数据结构及实现）
- [我以为我对Mysql索引很了解，直到我遇到了阿里的面试官](https://zhuanlan.zhihu.com/p/78982303) （mysql索引常见问题）
- [MySQL索引连环18问！](https://zhuanlan.zhihu.com/p/364041898) （很全，基本都是常用的）

### 1.1 索引是什么？

索引是一种数据结构，协助快速查询和更新数据库表中的数据。

索引也是一种特殊的文件，包含数据库表里所有记录的引用指针。

可以类比字典，有拼音或者笔画的快速检索，找到对应的页码，打开后即可知道某一个key的全部值信息。

### 1.2 索引的优缺点？

优点：

- 大大加快检索速度。（创建索引最主要原因）
- 使用索引，在查询过程中使用优化隐藏器，提高系统性能。

缺点：

- 时间方面：创建索引和维护索引需要耗费时间，对索引数据增删改、索引也要维护，降低增删改效率。
- 空间方面：索引占用物理空间。

### 1.3 MySQL的索引类型

存储结构划分：BTree索引（B-Tree或B+Tree索引），Hash索引，full-index全文索引，R-Tree索引。

应用层次来分：

- 普通索引：即一个索引只包含单个列，一个表可以有多个单列索引
- 唯一索引：索引列的值必须唯一，但允许有空值
- 复合索引：多列值组成一个索引，专门用于组合搜索，其效率大于索引合并
- 聚簇索引和非聚簇索引（下边介绍）

根据数据的物理顺序与键值的逻辑（索引）顺序关系： 聚集索引，非聚集索引

### 1.4 Mysql 索引底层数据结构选型（为什么索引结构默认使用B+Tree，而不是B-Tree，Hash，二叉树，红黑树？）

数据结构选型以如下图的user表进行分析：

<img src="https://gitee.com/koala010/typora/raw/master/img/20210626100234.png" alt="user表部分数据示例" style="zoom: 50%;" />

#### 哈希表

哈希表可以进行数据的快速检索。

哈希算法：也叫散列算法，就是把任意值(key)通过哈希函数变换为固定长度的 key 地址，通过这个地址进行数据查询的数据结构。

<img src="https://gitee.com/koala010/typora/raw/master/img/20210626100002.png" alt="哈希表原理" style="zoom:67%;" />

如果需要检索`id=7`的数据，sql如下：

```sql
select * from user where id=7;
```

哈希算法快速检索数据的计算过程：首先计算存储 id=7 的数据的物理地址 addr=hash(7)=4231，而 4231 映射的物理地址是 0x77，0x77 就是 id=7 存储的额数据的物理地址，通过该独立地址可以找到对应 user_name='g'这个数据。

但是hash算法可能出现**碰撞问题**，即hash函数可能计算相同的key值，不同的key映射到了同一个结果。解决碰撞问题的常见方法是**链地址法**：碰撞数据使用链表连接，计算hash值后，判断该值如果有碰撞，遍历到链表，直到找到真正的key所对应的数据为止。

![hash碰撞-链地址法](https://gitee.com/koala010/typora/raw/master/img/20210626101110.png)

从算法时间复杂度分析来看，哈希算法时间复杂度为 O（1），检索速度非常快。比如查找 id=7 的数据，哈希索引只需要计算一次就可以获取到对应的数据，检索速度非常快。但是 Mysql 并没有采取哈希作为其底层算法。

```
select * from user where id >3;
```

即hash算法针对范围查找效率太低，需要把所有数据找出来加载到内存，然后再在内存里筛选筛选目标范围内的数据。

**总结：hash表可以快速检索数据，但是范围查找效率低。所以不适合作为MySQL的索引数据结构**。

#### 二叉查找数（BST）

二叉查找树支持快速查找数据，时间复杂度为O(logn)。如下图所示，只需计算三次即可找到`id=7`的数据。需要考虑其是否能解决hash表范围查找效率低的问题。

<img src="https://gitee.com/koala010/typora/raw/master/img/20210626101831.png" alt="image-20210626101831123" style="zoom:67%;" />

二叉查找数按序排列（从左到右升序），如果想找到`id>5`的数据，只需要找到**节点6及其右子树**的数据即可，范围查找比较容易实现。

**普通二叉树的致命缺点**：极端情况下会退化为线性链表，时间复杂度为O(n)，性能急剧下降，如下图：

<img src="https://gitee.com/koala010/typora/raw/master/img/20210626102314.png" alt="二叉树退化为线性链表"  />

数据库表的主键id一般为自增，上述的线性结构查找问题必然出现，而且频率还很高。

**总结：二叉查找数查询效率高，而且可以解决hash表范围查询效率低的问题，但是会频繁出现不平衡退化问题导致查询效率低的问题，所以不适合作为MySQL索引的数据结构**。

#### 红黑树

二叉查找树不平衡，可以通过树节点的自动旋转和调整来解决，从而保证二叉树的查找性能。最常见的思路是平衡二叉树和红黑树。

红黑树可以自动调整树的结构，当二叉树不平衡时，红黑树自动左右旋转和节点变色，保持基本平衡，时间复杂度为O(logn)，查询效率不会降低。如下图，红黑树查找`id=7`的节点只需要查找4次：

<img src="https://gitee.com/koala010/typora/raw/master/img/20210626105121.png" alt="红黑树查找节点" style="zoom: 80%;" />

**但是红黑树也有缺点，即“右倾”现象**（参看下图），虽然没有二叉树退化那么夸张，但数据库主键基本都是自增，面对成千上百万的数据，这查询效率可想而知。

![红黑树“右倾”现象](https://gitee.com/koala010/typora/raw/master/img/20210626105920.png)

**总结：红黑树查询效率与二叉查找树相似，极端退化情况比平衡二叉树好，但是也没能达到预期，所以不适合MySQL索引的数据结构**。

#### 平衡二叉树（AVL）

平衡二叉树，是绝对平衡的。

AVL 树顺序插入 1~7 个节点，查找 id=7 所要比较节点的次数为 3。

AVL 树插入 1~16 个节点，查找 id=16 需要比较的节点数为 4。从查找效率而言，AVL 树查找的速度要高于红黑树的查找效率（AVL 树是 4 次比较，红黑树是 6 次比较）。

从树的形态看来，AVL 树不存在红黑树的“右倾”问题。大量的顺序插入不会导致查询性能的降低，这从根本上解决了红黑树的问题。

**AVL 树的优点**：

- 查询效率高O(logn)，不存在极端情况。
- 可以进行范围查找和排序。

**但是为什么不选取AVL树作为MySQL索引的数据结构**？

**主要是磁盘IO因素的影响**。如果使用AVL树，每一个树节点，只存储一个数据。如果查询`id=7`的数据，需要比对三次树节点，即进行三次磁盘IO操作，如果数据量大了，那磁盘IO的次数会很高，消耗大量的时间。

所以，**设计数据库索引的时候，还需要考虑怎么尽可能减少磁盘的IO次数**。

**磁盘读取1B和1KB的数据消耗的时间基本是一样的，所以可以在一个节点存储更多的数据，即一次磁盘IO读取更多数据，即可解决问题。所以就考虑到了B树**。

#### B树（B-树）

B树的理解参考：[平衡二叉树、B树、B+树、B*树 理解其中一种你就都明白了](https://zhuanlan.zhihu.com/p/27700617)

B树，平衡多路查找树，又称B-树。

**如果每个节点限制最多存储两个key（即二叉树），一个节点如果超过两个key会自动分裂。**

比如下面这个存储了 7 个数据 B 树，只需要查询两个节点就可以知道 id=7 这数据的具体位置，也就是两次磁盘 IO 就可以查询到指定数据，优于 AVL 树。

<img src="https://gitee.com/koala010/typora/raw/master/img/20210626112940.png" alt="B树7节点（限制单节点key=2）" style="zoom:80%;" />

如果是一个存储了 16 个数据的 B 树，同样每个节点最多存储 2 个 key，查询 id=16 这个数据需要查询比较 4 个节点，也就是经过 4 次磁盘 IO。看起来查询性能与 AVL 树一样。

<img src="https://gitee.com/koala010/typora/raw/master/img/20210626115100.png" alt="B树16节点（限制单节点key=2）" style="zoom:80%;" />

**如果限制每个节点可以存储6个key。**

一个存储了 7 个数据的 B 树，查询 id=7 这个数据所要进行的磁盘 IO 为 2 次。

<img src="https://gitee.com/koala010/typora/raw/master/img/20210626115252.png" alt="B树7节点（限制单节点key=6）" style="zoom:80%;" />



一个存储了 16 个数据的 B 树，查询 id=7 这个数据所要进行的磁盘 IO 为 2 次。相对于 AVL 树而言磁盘 IO 次数降低为一半。

![B树16节点（限制单节点key=6）](https://gitee.com/koala010/typora/raw/master/img/20210626115329.png)

**B 树作数据库索引优点**：

- 优秀检索速度，时间复杂度：B 树的查找性能等于 O（h*logn），其中 h 为树高，n 为每个节点关键词的个数；
- 尽可能少的磁盘 IO，加快了检索速度；
- 可以支持范围查找。

B-Tree能加快数据的访问速度，因为存储引擎不再需要进行全表扫描来获取数据，但是数据分布在各个节点之中，每个节点存储的数据量是有限的，MySQL希望一个节点可以尽可能多的存储数据，因此采用了B+树。

#### B+树

- B树一个节点存储的是数据，一个节点中存储不了太多数据；而B+树非叶子节点存储的是地址，叶子结点存储的是数据，可以存储更多数据。
- B+数叶子结点采用链表串联，更便于范围查找。而B树需要中序遍历。

![B+树](https://gitee.com/koala010/typora/raw/master/img/20210626115803.png)

###  1.5 Innodb 引擎和 Myisam 引擎对索引的实现

Myisam 虽然数据查找性能极佳，但是不支持事务处理。Innodb 最大的特色就是支持了 ACID 兼容的事务功能，而且他支持行级锁。Mysql 建立表的时候就可以指定引擎。B+树作为 Mysql 的索引的数据结构非常合适，那么两种引擎是怎么实现的呢？

在执行建表语句并指定引擎后，Innodb 生成的文件有：

- frm:创建表的语句
- ibd:表里面的数据+索引文件

Myisam 生成的文件有:

- frm:创建表的语句
- MYD:表里面的数据文件（myisam data）
- MYI:表里面的索引文件（myisam index）

从生成的文件看来，这两个引擎底层数据和索引的组织方式并不一样，MyISAM 引擎把数据和索引分开了，一人一个文件，这叫做**非聚集索引方式**；Innodb 引擎把数据和索引放在同一个文件里了，这叫做**聚集索引方式**。

接下来从底层实现的角度分析。

**MyISAM 引擎的底层实现（非聚集索引方式）**

MyISAM 用的是非聚集索引方式，即数据和索引落在不同的两个文件上。MyISAM 在建表时**以主键作为 KEY 来建立主索引 B+树，树的叶子节点存的是对应数据的物理地址**。**通过这个物理地址后，就可以到 MyISAM 数据文件中直接定位到具体的数据记录了**。

<img src="https://gitee.com/koala010/typora/raw/master/img/20210626154923.png" alt="MyISAM 引擎的底层实现" style="zoom: 50%;" />

在为其他字段添加索引时，同样会生成对应的索引树，检索方式与上述相同。



**Innodb 引擎的底层实现（聚集索引方式）**

InnoDB 是的主键索引是聚集索引方式，数据和索引都存储在同一个文件里。首先 InnoDB 会根据主键 ID 作为 KEY 建立索引 B+树，而 B+树的叶子节点存储的是主键 ID 对应的数据。在根据主键ID查询时，会查询这颗主键ID的索引树，找到对应叶子结点的数据。

建表的时候，InnoDB就会建好主键ID的索引树，这也是为什么 Mysql 在建表时要求必须指定主键的原因。

在为其他字段建立索引时，非叶子结点存储当前字段的key，**叶子结点存储主键的key**。得到主键key后，才会在主键索引树中找到当前字段所对应的数据。

<img src="https://gitee.com/koala010/typora/raw/master/img/20210626155854.png" alt="Innodb 引擎的底层实现" style="zoom:50%;" />



**为什么 InnoDB 只在主键索引树的叶子节点存储了具体数据，但是其他索引树却不存具体数据呢，而要多此一举先找到主键，再在主键索引树找到对应的数据呢**？

因为 InnoDB 要节省存储空间。一个表里可能有很多个索引，如果给每个加了索引的字段生成索引树，都存储了具体数据，那么这个表的索引数据文件就变得非常巨大（数据极度冗余了）。从节约磁盘空间的角度来说，没有必要，通过这种看似“多此一举”的步骤，在牺牲较少查询的性能下节省了巨大的磁盘空间。

**为什么InnoDB 和MyISAM 对比，MyISAM 查询性能更好？**

从上面索引文件数据文件的设计来看也可以看出原因：

- **MyISAM 直接找到物理地址后就可以直接定位到数据记录**。
- **InnoDB 查询到叶子节点后，还需要再查询一次主键索引树，才可以定位到具体数据**。

等于 MyISAM 一步就查到了数据，但是 InnoDB 要两步，所以 MyISAM 查询性能更高。

### 1.6 InnoDB中一棵B+树能存多少行数据？

**约 2 千万**

参看：

- [面试题：InnoDB中一棵B+树能存多少行数据？](https://zhuanlan.zhihu.com/p/67982911) 
- [面试题：mysql 一棵 B+ 树能存多少条数据？](https://zhuanlan.zhihu.com/p/379092178)

### 1.7 聚簇索引和非聚簇索引

> 聚簇索引是叶子结点存储整行数据，即数据与索引存储在一块，找到索引也就找到了数据。InnoDB的主键索引是聚簇索引。

> 非聚簇索引是叶子结点存储了主键的值，也被称为二级索引。

- 非聚集索引与聚集索引的区别在于**非聚集索引的叶子节点不存储表中的数据，而是存储该列对应的主键**。
- 对于InnoDB来说，非主键的索引查到了主键的值，还需要去主键索引树再次查找数据，称这个过程为**回表**。
- 通常情况下， 主键索引（聚簇索引）查询只会查一次，而非主键索引（非聚簇索引）需要回表查询多次。
- MyISAM无论主键索引还是二级索引都是非聚簇索引，而InnoDB的主键索引是聚簇索引，二级索引是非聚簇索引。我们自己建的索引基本都是非聚簇索引。

### 1.8 非聚簇索引一定会回表查询吗？（覆盖索引）

不一定，这涉及到查询语句所要求的字段是否全部命中了索引，如果全部命中了索引，那么就不必再进行回表查询。一个索引包含（覆盖）所有需要查询字段的值，被称之为"覆盖索引"。

例子：假设在学生表的成绩（`score`）上建立了索引，那么当进行`select score from student where score > 90`的查询时，在索引的叶子节点上，已经包含了score 信息，不会再次进行回表查询。

### 1.9 联合索引是什么？为什么需要注意联合索引中的顺序？

> 联合索引：使用多个字段同时建立一个索引。

在联合索引中，只有按照建立索引时的字段顺序使用，才能命中。

例子：假设建立“name，age，school”的联合索引，那么索引的排序为: 先按照name排序进行索引，如果name相同，则按照age排序索引，如果age的值也相等，则按照school进行排序索引。

因此在建立联合索引的时候应该注意索引列的顺序，一般情况下，将查询需求频繁或者字段选择性高的列放在前面。

### 1.10 MySQL的最左前缀原则?

最左前缀原则就是最左优先，在创建多列索引时，要根据业务需求，where子句中使用最频繁的一列放在最左边。 mysql会一直向右匹配直到遇到范围查询(>、<、between、like)就停止匹配。

例子：比如`a = 1 and b = 2 and c > 3 and d = 4 `如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。

`=`和`in`可以乱序，比如`a = 1 and b = 2 and c = 3` 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会优化成索引可以识别的形式。

### 1.11 前缀索引？

可能出现建立索引字段非常长的情况，这样既占用内存空间，也不利于维护。所以可以选取字段前面的公共部分作为一个索引，大大提升检索效率。但是`ORDER BY`和`GROUP BY`不支持前缀索引 。

流程：

1. 计算完整列的选择性 :`select count(distinct col_1)/count(1) from table_1`
2. 计算不同前缀长度的选择性 :`select count(distinct left(col_1,4))/count(1) from table_1`
3. 找到最优长度之后，创建前缀索引 :`create index idx_front on table_1 (col_1(4))`

注意事项：

- 前缀索引是一种能使索引更小，更快的有效办法，但另一方面也有其缺点：mysql无法使用其前缀索引做ORDER BY和GROUP BY，也无法使用前缀索引做覆盖扫描
- 要明确使用前缀索引的目的与优势
  - 大大节约索引空间，从而提高索引效率
  - 对于 BOLB 、 TEXT 或者很长的 VARCHAR 类型的列，必须使用前缀索引,因为 MySQL 不允许索引这些列的完整长度
- 前缀索引会降低索引的选择性
  - 关于索引的选择性，它是指不重复的索引值（也称为基数cardinality)和数据表的记录总数的比值，范围从1/(数据表记录总数)到1之间。索引的选择性越高则查询效率越高，因为选择性高的索引可以让MySQL在查找时过滤掉更多的行。选择性为1的索引叫唯一索引，这是最好的索引选择性，性能也是最好的。
- 真正的难点在于：要选择足够长的前缀以保证较高的选择性，同时又不能太长， 前缀的长度应该使前缀索引的选择性接近索引整个列，即前缀的基数应该接近于完整列的基数

前缀索引分析可参看：[MySQL 前缀索引](https://www.cnblogs.com/niuben/p/13188277.html) 

### 1.12 索引下推？

> MySQL 5.6引入了索引下推优化。默认开启，使用`SET optimizer_switch = ' index_condition_pushdown=off ';`可以将其关闭。有了索引下推优化，可以在**减少回表次数**。

官方解释：

在 people_table中有一个二级索引(zipcode，lastname，firstname)，查询语句：`SELECT * FROM people WHERE zipcode='95054' AND lastname LIKE '%etrunia%' AND address LIKE '%Main Street%' ;`

如果没有使用索引下推技术，则MySQL会通过`zipcode='95054'`从存储引擎中查询对应的数据，返回到MySQL服务端（**回表**），然后MySQL服务端基于`lastname LIKE '%etrunia%' AND address LIKE '%Main Street%'`来判断数据是否符合条件。

如果使用了索引下推技术，则MYSQL首先会返回符合`zipcode='95054'`的索引，然后根据`lastname LIKE '%etrunia%' AND address LIKE '%Main Street%'`来判断索引是否符合条件。如果符合条件，则根据该索引来定位对应的数据，如果不符合，则直接reject掉。

注意：**在InnoDB中索引下推只对二级索引有效**。

### 1.13 怎么查看MySQL语句有没有用到索引？

可以通过explain查看sql语句的执行计划，通过执行计划来分析索引使用情况，只需要将explain添加在sql语句之前即可。

表中的索引：

![表中的索引](https://gitee.com/koala010/typora/raw/master/img/20210626214501.png)

通过explain查看sql是否用到索引：

![explain查看sql的结果](https://gitee.com/koala010/typora/raw/master/img/20210626214556.png)

- type 的信息很明显的体现是否用到索引，它提供了判断查询是否高效的重要依据依据，如const(主键索引或者唯一二级索引进行等值匹配的情况下)，ref(普通的⼆级索引列与常量进⾏等值匹配)，index(扫描全表索引的覆盖索引) 。性能如下：`ALL < index < range ~ index_merge < ref < eq_ref < const < system`。 `ALL` 类型因为是全表扫描, 因此在相同的查询条件下, 它是速度最慢的. 而 `index` 类型的查询虽然不是全表扫描, 但是它扫描了所有的索引, 因此比 ALL 类型的稍快。
- select_type：select关键字对应的那个查询的类型，如SIMPLE，PRIMARY，SUBQUERY，DEPENDENT，SNION 。
- table：每个查询对应的表名 。
- possible_key：查询中可能用到的索引
- key：此字段是 MySQL 在当前查询时所真正使用到的索引。
- filtered：查询器预测满足下一次查询条件的百分比 。
- rows: 显示MySQL认为它执行查询时必须检查的行数。这个值非常直观显示 SQL 的效率好坏, 原则上 rows 越少越好。
- extra：表示额外信息。

### 1.14 什么情况下不走索引（索引失效）？

1. **使用`!=` 或者 `<>` 导致索引失效**。
2. **类型不一致导致的索引失效**。
3. **函数导致的索引失效**。如：`SELECT * FROM user WHERE DATE(create_time) = '2020-09-03';` 如果`create_time`添加了索引，索引会失效。
4. **运算符导致的索引失效**。如：`SELECT * FROM user WHERE age - 1 = 20;` 如果对列进行了（+，-，*，/，!）, 那么都将不会走索引。
5. **`OR`引起的索引失效**。OR导致索引是在特定情况下的，并不是所有的OR都是使索引失效，如果OR连接的是同一个字段，那么索引不会失效，反之`OR`右侧字段索引失效。
6. **模糊搜索导致的索引失效**。当`%`放在匹配字段前是不走索引的，放在后面才会走索引。
7. **`NOT IN`、`NOT EXISTS`导致索引失效**。

### 1.15 为什么官方建议使用自增长数字主键作为索引？

**建议使用有序的自增ID作为主键**

提高效率主要体现在：

- 提高范围查询效率；
- 增加排序效率；
- 提高扫表能力,顺序访问。

结合B+树的特点，一个表有多少个索引就会有多少颗B+树，MySQL的数据都是按顺序保存在树的叶子结点上的。

mysql 在底层又是以数据页为单位来存储数据的，一个数据页大小默认为 16k（可以自定义大小）。如果一个数据页存满了，mysql 就会去申请一个新的数据页来存储数据。

- 如果主键为自增 id 的话，mysql 在写满一个数据页的时候，直接申请另一个新数据页接着写就可以了。
- 如果主键是非自增 id，为了确保索引有序，mysql 就需要将每次插入的数据都放到合适的位置上。当往一个快满或已满的数据页中插入数据时，新插入的数据会将数据页写满，mysql 就需要申请新的数据页，并且把上个数据页中的部分数据挪到新的数据页上。这就造成了页分裂，这个大量移动数据的过程是会严重影响插入效率的。

![InnoDB逻辑结构](https://gitee.com/koala010/typora/raw/master/img/20210626221023.png)

**在满足业务需求的情况下，尽量使用占空间更小的主键**

- 主键占用空间越大，每个页存储的主键个数越少，路树就越少，B+树的深度会边长，导致IO次数会变多。
- 普通索引的叶子节点上保存的是主键 id 的值，如果主键 id 占空间较大的话，那将会成倍增加 mysql 空间占用大小。

![索引流程图](https://gitee.com/koala010/typora/raw/master/img/20210626220947.png)

**总结：使用自增主键可以提高效率（范围查询、排序、扫表），而且自增数字占用更小的空间，可以存储更多的数据。**

### 1.16 如何创建索引？

**1、在执行`CREATE TABLE`时创建索引**

```sql
CREATE TABLE user_index2 (
 id INT auto_increment PRIMARY KEY,
 name VARCHAR (16),
 id_card VARCHAR (18),
 information text,
 INDEX (id_card)
);
```

**2、使用`ALTER TABLE`命令去增加索引**

```sql
ALTER TABLE table_name ADD INDEX index_name (column_list);
```

ALTER TABLE用来创建普通索引、UNIQUE索引或PRIMARY KEY索引。

### 1.17 创建索引时需要注意什么？建索引的原则有哪些？

注意事项：

- **较频繁的作为查询条件的字段应该创建索引**。
- **唯一性太差的字段不适合单独创建索引**，即使该字段频繁作为查询条件。
- **更新非常频繁的字段不适合创建索引**。
- **非空字段**：应该指定列为NOT NULL，除非你想存储NULL。在mysql中，含有空值的列很难进行查询优化，因为它们使得索引、索引的统计信息以及比较运算更加复杂。你应该用0、一个特殊的值或者一个空串代替空值。
- **取值离散大的字段**（变量各个取值之间的差异程度），将其列放到联合索引的前面，可以通过count()函数查看字段的差异值，返回值越大说明字段的唯一值越多字段的离散程度高。
- **索引字段越小越好**：数据库的数据存储以页为单位一页存储的数据越多一次IO操作获取的数据越大效率越高。

原则：

- **最左前缀匹配原则**。在创建多列索引时，要根据业务需求，where子句中使用最频繁的一列放在最左边。
- **`=`和`in`可以乱序**。比如`a = 1 and b = 2 and c = 3` 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式。
- **尽量选择区分度高的列作为索引**。区分度的公式是`count(distinct col)/count(*)`，表示字段不重复的比例，比例越大我们扫描的记录数越少，唯一键的区分度是1。
- **索引列不能参与计算**。计算代表逻辑计算和使用函数，会使索引失效。
- **尽量的扩展索引，不要新建索引**。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。

### 1.18 使用索引查询一定能提高查询的性能吗？

使用索引查询不一定能提高查询性能.

**通常通过索引查询数据比全表扫描要快，但是我们也必须注意到使用的代价**。

索引需要空间来存储，也需要定期维护， 每当有记录在表中增减或索引列被修改时，索引本身也会被修改。

这意味着每条记录的`INSERT`，`DELETE`，`UPDATE`将为此多付出4，5 次的磁盘I/O。 因为索引需要额外的存储空间和处理，那些不必要的索引反而会使查询反应时间变慢。

索引范围查询(INDEX RANGE SCAN)适用于两种情况:

1. 基于一个范围的检索，一般查询返回结果集小于表中记录数的30%。

2. 基于非唯一性索引的检索。

否则索引范围查询的效率会大大降低。

### 索引参考

- [深入理解 Mysql 索引底层原理](https://zhuanlan.zhihu.com/p/113917726) （mysql索引的底层数据结构及实现）
- [我以为我对Mysql索引很了解，直到我遇到了阿里的面试官](https://zhuanlan.zhihu.com/p/78982303) （mysql索引常见问题）
- [MySQL索引连环18问！](https://zhuanlan.zhihu.com/p/364041898) （很全，基本都是常用的）
- [mysql前缀索引的索引选择性](https://blog.csdn.net/dhrome/article/details/72853153)
- [MySQL 前缀索引](https://www.cnblogs.com/niuben/p/13188277.html) 
- [面试题：InnoDB中一棵B+树能存多少行数据？](https://zhuanlan.zhihu.com/p/67982911) 
- [面试题：mysql 一棵 B+ 树能存多少条数据？](https://zhuanlan.zhihu.com/p/379092178)
- [mysql 如何查看是否有用到索引_mysql 如何查看sql查询是否用到索引](https://blog.csdn.net/weixin_33816685/article/details/113276900)
- [mysql innodb为什么建议使用自增数字作为主键？](https://www.cnblogs.com/kancy/p/13458991.html)
- [MySQL 如何创建索引？怎么优化？](https://www.cnblogs.com/lfs2640666960/p/9147768.html)

## 2. 存储引擎

### 2.1 Mysql 中 MyISAM 和 InnoDB 的区别有哪些？

- **InnoDB 支持事务，MyISAM 不支持事务**。
  - 这是 MySQL 将默认存储引擎从 MyISAM 变成 InnoDB 的重要原因之一。
  - 对于 InnoDB 每一条 SQL 语句都默认封装成事务进行提交，这样就会影响速度，优化速度的方式是将多条 SQL 语句放在 begin 和 commit 之间，组成一个事务。
- **InnoDB 支持外键，而 MyISAM 不支持**。
  - 对一个包含外键的 InnoDB 表转为 MYISAM 会失败。
- **InnoDB 主键索引是聚集索引，非主键索引是非聚集索引；MyISAM 都是是非聚集索引**。
  - 聚簇索引的文件存放在主键索引的叶子节点上，因此 InnoDB 必须要有主键，通过主键索引效率很高。但是辅助索引需要两次查询，先查询到主键，然后再通过主键查询到数据。因此，主键不应该过大，因为主键太大，其他索引也都会很大。
  - MyISAM 是非聚集索引，数据文件是分离的，索引保存的是数据文件的指针。主键索引和辅助索引是独立的。
- **InnoDB 不保存表的具体行数**，执行 `select count(*) from table` 时需要全表扫描。而**MyISAM 用一个变量保存了整个表的行数**，执行上述语句时只需要读出该变量即可，速度很快。
- **InnoDB 最小的锁粒度是行锁，MyISAM 最小的锁粒度是表锁**。
  - MyISAM 一个更新语句会锁住整张表，导致其他查询和更新都会被阻塞，因此并发访问受限。这也是 MySQL 将默认存储引擎从 MyISAM 变成 InnoDB 的重要原因之一；
- **存储结构不同**。
  - MyISAM在磁盘上存储成三个文件，数据和索引分离。
  - InnoDB在磁盘上存储成两个文件，数据和索引存储在一个文件。
- **表的主键策略不同**。
  - MyISAM：允许没有任何索引和主键的表存在，索引都是保存行的地址。
  - InnoDB：如果没有设定主键或者非空唯一索引，**就会自动生成一个6字节的主键(用户不可见)**，数据是主索引的一部分，附加索引保存的是主索引的值。

更多详细的区别，可参考：[Mysql 中 MyISAM 和 InnoDB 的区别有哪些？](https://www.zhihu.com/question/20596402/answer/211492971)

### 2.2 如何选择存储引擎？

- 是否要支持事务，如果要请选择 InnoDB，如果不需要可以考虑 MyISAM。
- 如果表中绝大多数都只是读查询（一般R/W > 100:1且update相对较少），可以考虑 MyISAM；如果既有读写也挺频繁，请使用InnoDB。
- 系统奔溃后，MyISAM恢复起来更困难，能否接受，不能接受就选 InnoDB。
- 表数据量超过1000万，并发高，请使用InnoDB；如果数据量很小，可以考虑MyISAM。
- MySQL5.5版本开始Innodb已经成为Mysql的默认引擎(之前是MyISAM)，说明其优势是有目共睹的。如果你不知道用什么存储引擎，那就用InnoDB，至少不会差。

### 2.3 InnoDB四大特性

#### 1、插入缓冲（insert buffer）

> 插入缓冲（Insert Buffer/Change Buffer）：提升插入性能，change buffering是insert buffer的加强，insert buffer只针对insert有效，change buffering对insert、delete、update(delete+insert)、purge都有效

使用插入缓冲的条件：

- 非聚集索引（辅助索引）
- 非唯一索引

Change buffer是作为buffer pool中的一部分存在。`Innodb_change_buffering`参数缓存所对应的操作(update会被认为是delete+insert)：

- `all`: 默认值，缓存insert, delete, purges操作
- `none`: 不缓存
- `inserts`: 缓存insert操作
- `deletes`: 缓存delete操作
- `changes`: 缓存insert和delete操作
- `purges`: 缓存后台执行的物理删除操作

`innodb_change_buffer_max_size`参数：控制使用的大小，默认25%，最大可设置50%，如果mysql实例中有大量的修改操作，可考虑增大该参数；

对满足插入缓存条件的插入，每一次的插入不是写到索引页中，而是：

1. 会先判断插入的非聚集索引页是否在缓冲池中，如果在直接插入；
2. 如果不在，则先放到insert buffer中，再按照一定的频率进行合并操作，再写会磁盘；
3. 通常可以将多个插入合并到一个操作中，目的是为了减少随机IO带来的性能损耗；

这样通常能将多个插入合并到一个操作中，目的还是为了**减少随机IO带来性能损耗**。

**上面提过在一定频率下进行合并，那所谓的频率是什么条件**？

1. 辅助索引页被读取到缓冲池中。正常的select先检查Insert Buffer是否有该非聚集索引页存在，若有则合并插入。
2. 辅助索引页没有可用空间。空间小于1/32页的大小，则会强制合并操作。
3. Master Thread 每秒和每10秒的合并操作。

insert buffer的数据结构是一颗B+树；

- 全局只有一颗insert buffer B+树，负责对所有表的辅助索引进行insert buffer；
- 这颗B+树放在共享表空间中，试图通过独立表空间ibd文件恢复表中数据时，往往会导致check table失败，因为表中的辅助索引中的数据可能还在insert buffer中，也就是共享表空间中，所以ibd文件恢复后，还需要repair table操作来重建表上所有的辅助索引；

#### 2、二次写（double write）

1. doublewrite缓存位于系统表空间的存储区域，用来缓存innodb的数据页从innodb buffer pool中flush之后并写入到数据文件之前；
2. 当操作系统或数据库进程在数据页写入磁盘的过程中崩溃，可以在doublewrite缓存中找到数据页的备份，用来执行crash恢复；
3. 数据页写入到doublewrite缓存的动作所需要的io消耗要小于写入到数据文件的消耗，因为此写入操作会以一次大的连续块的方式写入；

![二次写过程](https://gitee.com/koala010/typora/raw/master/img/20210627104446.png)

从上图可知：

1. 内存中doublewrite buffer大小2M；物理磁盘上共享表空间中连续的128个页，也就是2个区（extent）大小同样为2M
2. 对缓冲池脏页进行刷新时，不是直接写磁盘。流程：
   1. 通过memcpy()函数将脏页先复制到内存中的doublewrite buffer
   2. 通过doublewrite分两次，每次1M顺序的写入共享表空间的物理磁盘上。这个过程中，doublewrite页是连续的，因此这个过程是顺序的，所以开销并不大；
   3. 完成doublewrite页的写入后，再将doublewrite buffer中的页写入各个表空间文件中，此时写入是离散的，可能会较慢；
   4. 如果操作系统在第三步的过程中发生了崩溃，在恢复过程中，可以从共享表空间中的doublewrite中找到该页的一个副本，将其复制到表空间文件中，再应用重做日志；

#### 3、自适应hash索引（ahi）

> innodb存储引擎会监控对表上二级索引的查找，如果发现某二级索引被频繁访问，此索引成为热数据，建立hash索引以提升查询速度，此建立是自动建立哈希索引，故称为自适应哈希索引（adaptive hash index）。

该属性通过`innodb_adapitve_hash_index`开启，也可以通过`—skip-innodb_adaptive_hash_index`参数关闭

注意事项：

- 自适应哈希索引会占用innodb buffer pool
- 只适合搜索等值（=）的查询，对于范围查找等操作，是不能使用的
- 极端情况下，自适应hash索引才有比较大的意义，可以降低逻辑读

#### 4、预读(read ahead)

> **extent 定义**：表空间（tablespace 中的一组 page）

InnoDB使用两种预读算法来提高I/O性能：线性预读（linear read-ahead）和随机预读（randomread-ahead）。

- 线性预读：以extent为单位，将下一个extent提前读取到buffer pool中；
- 随机预读：以extent中的page为单位，将当前extent中的剩余的page提前读取到buffer pool中；

线性预读一个重要参数：innodb_read_ahead_threshold，控制什么时间（访问extent中多少页的阈值）触发预读；

- 默认：56，范围：0～64，值越高，访问模式检查越严格；
- 没有该变量之前，当访问到extent最后一个page时，innodb会决定是否将下一个extent放入到buffer pool中；

随机预读说明：

- 当同一个extent的一些page在buffer pool中发现时，innodb会将extent中剩余page一并读取到buffer pool中；
- 随机预读给innodb code带来一些不必要的复杂性，性能上也不稳定，在5.5版本已经废弃，如果启用，需要修改变量：innodb_random_read_ahead为ON；

参考：[InnoDB四大特性](https://zhuanlan.zhihu.com/p/109528131)

## 3. 事务

### 3.1 什么是事务？

> 数据库事务指的是一组sql语句组成的数据库逻辑处理单元，在这组的sql操作中，要么全部执行成功，要么全部执行失败。

例子：转账。用户A要转账给用户B，要经历如下过程：用户A转账扣钱->用户B收账加钱，为了**保证数据的一致性**，要采用事务。两步操作都要成功才能成功，只要有一步出错，全都执行失败，即回滚。

### 3.2 事务的特性（ACID）

- 原子性（Atomicity）：事务的原子性操作，数据操作要么全部成功，要么全部失败。
  - 基于日志的`Redo/Undo`机制
- 一致性（Consistent）：事务执行前后的状态要一致，可理解为数据一致性。
- 隔离性（Isalotion）：事务之前相互隔离，不受影响，与事务的隔离级别密切相关。
  - 数据库系统提供-定的隔离机制，事务处理过程中的中间状态对外部是不可见的，保证事务在不受外部并发操作影响的“独立”环境执行。
- 持久性（Durable）：事务完成之后，它对于数据的修改是永久性的（持久化到数据库），即使出现系统故障也能够保持。

原子性、隔离性、持久性都是为了保障一致性而存在的，一致性也是最终的目的。

### 3.3 什么是`Redo/Undo`机制？

Redo log用来记录某数据块被修改后的值，可以用来恢复事务已提交但还未持久化到数据库的数据；Undo log是用来记录数据更新前的值，保证数据更新失败能够回滚。

场景：假如某个时刻数据库崩溃，在崩溃之前有事务A和事务B在执行，事务A已经提交，而事务B还未提交。当数据库重启进行 crash-recovery 时，就会通过Redo log将已经提交事务的更改写到数据文件，而还没有提交的就通过Undo log进行roll back。

### 3.4 什么是脏读？幻读？不可重复读？（事务可能导致的问题）

#### 脏读

脏读指的是读到了其他事务未提交的数据，未提交意味着这些数据可能会回滚，也就是可能最终不会存到数据库中，也就是不存在的数据。读到了并一定最终存在的数据，这就是脏读。

#### 幻读

幻读是针对数据**插入（`INSERT`）**操作来说的。假设事务A修改了某些行的数据，但未提交，此时事务B插入了与事务A更改前记录相同的记录行，并先于事务A提交。那么在事务A查询时，会发现好像刚才更改对某些数据未起作用，但其实是事务B刚刚插入进来的，感觉除了幻觉，称之为幻读。

#### 不可重复读

对比可重复读，不可重复读指的是在同一事务内，不同的时刻读到的同一批数据可能是不一样的，可能会受到其他事务的影响，比如其他事务改了这批数据并提交了。通常针对数据**更新（`UPDATE`）**操作。

#### 可重复读（正常情况）

可重复读指的是在同一个事务内，最开始读到的数据和事务结束前的任何时刻读到的同一批数据都是一致的。通常针对数据**更新（`UPDATE`）**操作。

### 3.5 事务隔离级别

#### 隔离级别解决的问题

事务隔离其实就是为了解决上面提到的脏读、不可重复读、幻读这几个问题。

| 隔离级别                     | 脏读   | 不可重复读 | 幻读   |
| ---------------------------- | ------ | ---------- | ------ |
| 读未提交（READ UNCOMMITTED） | 可能   | 可能       | 可能   |
| 读提交 （READ COMMITTED）    | 不可能 | 可能       | 可能   |
| 可重复读 （REPEATABLE READ） | 不可能 | 不可能     | 可能   |
| 串行化 （SERIALIZABLE）      | 不可能 | 不可能     | 不可能 |

从上往下，隔离强度逐渐增强，性能逐渐变差。采用哪种隔离级别要根据系统需求权衡决定，其中，**可重复读**是 MySQL 的默认级别。

#### 如何设置隔离级别

**查看当前数据库的隔离级别**：

```sql
# 查看事务隔离级别 5.7.20 之后
show variables like 'transaction_isolation';
SELECT @@transaction_isolation;

# 5.7.20 之前
SELECT @@tx_isolation;
show variables like 'tx_isolation';

#结果：
+---------------+-----------------+
| Variable_name | Value           |
+---------------+-----------------+
| tx_isolation  | REPEATABLE-READ |
+---------------+-----------------+
```

**查询当前有多少事务正在运行**：

```mysql
select * from information_schema.innodb_trx;
```

**修改数据库的隔离级别**：

```sql
set [作用域] transaction isolation level [事务隔离级别]，
SET [SESSION | GLOBAL] TRANSACTION ISOLATION LEVEL {READ UNCOMMITTED | READ COMMITTED | REPEATABLE READ | SERIALIZABLE}
```

- 作用域中GLOBAL 是全局的，而 SESSION 只针对当前回话窗口。
- 隔离级别是 {READ UNCOMMITTED | READ COMMITTED | REPEATABLE READ | SERIALIZABLE} 这四种，不区分大小写。

例子：比如下面这个语句的意思是设置全局隔离级别为读提交级别。

```mysql
set global transaction isolation level read committed;
```

#### 隔离级别分析

建立一张表用来测试

```mysql
CREATE TABLE `user` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `name` varchar(30) DEFAULT NULL,
  `age` tinyint(4) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4
```

初始只有一条记录：

![初始user表数据](https://gitee.com/koala010/typora/raw/master/img/20210627173121.png)

##### 读未提交

MySQL 事务隔离其实是依靠锁来实现的，加锁自然会带来性能的损失。而读未提交隔离级别是不加锁的，所以它的性能是最好的，没有加锁、解锁带来的性能开销。但有利就有弊，这基本上就相当于裸奔啊，所以它连脏读的问题都没办法解决。

任何事务对数据的修改都会第一时间暴露给其他事务，即使事务还没有提交。

做一个实验，先将全局隔离级别设置为**读未提交**：

```mysql
set global transaction isolation level read uncommitted;
```

设置完成后，只对之后新起的 session 才起作用，对已经启动 session 无效。如果用 shell 客户端那就要重新连接 MySQL，如果用 Navicat 那就要创建新的查询窗口。这时候再重新启动两个黑窗口进行模拟。

Mysql中开启事务有两种方式`begin/start transaction`，最后提交事务执行commit，或者回滚事务rollback。在执行`begin/start transaction`命令，它们并不是一个事务的起点，在执行完它们后的第一个sql语句，才表示事务真正的启动 。

**分析执行流程**：

1、在第一个黑窗口（事务A）中，执行`begin；`后；将`id=1`的数据行改为`name='duktig666'`。

```mysql
begin;
UPDATE user SET name='duktig666' WHERE id = 2;
```

2、在第二个黑窗口（事务B）中，执行执行`begin；`后；执行查询，观察数据。

```
begin;
SELECT * FROM user;
```

3、在第一个黑窗口（事务A）中将事务回滚，在第二个黑窗口（事务B）中再次执行查询，观察数据。

```mysql
rollback;
```

```
SELECT * FROM user;
commit;
```

![读未提交分析](https://gitee.com/koala010/typora/raw/master/img/20210627180646.png)

**总结**：

**读未提交，其实就是可以读到其他事务未提交的数据，但没有办法保证你读到的数据最终一定是提交后的数据，如果中间发生回滚，那就会出现脏数据问题，读未提交没办法解决脏数据问题。更别提可重复读和幻读了，想都不要想。**

##### 读提交

既然读未提交没办法解决脏数据问题，那么就有了读提交。

**读提交就是一个事务只能读到其他事务已经提交过的数据，也就是其他事务调用 commit 命令之后的数据**。那脏数据问题迎刃而解了。

读提交事务隔离级别是大多数流行数据库的默认事务隔离界别，比如 Oracle，但是不是 MySQL 的默认隔离界别。

继续验证，将事务隔离级别设置为**读提交**，然后重新打开两个mysql黑窗口。

```mysql
set global transaction isolation level read committed;
```

**分析执行流程**：

1、事务A开启事务，执行修改操作修改id=2的name：duktig->duktig666。

2、此时事务A未提交，事务B开启事务，执行查询操作，数据为duktig。

3、事务A提交，事务B再次执行查询操作，数据为duktig666。

具体代码参看“读未提交都差不多”。

![读提交分析](https://gitee.com/koala010/typora/raw/master/img/20210627184016.png)

在不同的时刻，查询出来的数据可能是不一致的，可能会受到其他事务的影响。

**总结**：

读提交解决了脏读的问题，但是无法做到可重复读，也没办法解决幻读。

##### 可重复读

可重复是对比不可重复而言的，上面说不可重复读是指同一事务不同时刻读到的数据值可能不一致。

**可重复读是指，事务不会读到其他事务对已有数据的修改，即使其他事务已提交，也就是说，事务开始时读到的已有数据是什么，在事务提交前的任意时刻，这些数据的值都是一样的。但是，对于其他事务新插入的数据是可以读到的，这也就引发了幻读问题**。

继续验证，需改全局隔离级别为可重复读级别，**将name重置为duktig**，并重新打开两个黑窗口。

```mysql
set global transaction isolation level repeatable read;
```

可对读提交的流程再执行一次，发现修改操作不会出现可重复读，即解决了可重复读（上述操作不在重复验证）。但是更新操作又引起了数据不一致（幻读）。

**分析验证流程**：

1、开启事务A，执行修改操作修改id=2的name：duktig->duktig666。

2、开启事务B，在事务A执行完update后，执行insert操作，插入记录“`name='duktig' age=23`”（这条数据和事务A修改前的name和age的值相同）。

```mysql
INSERT INTO user (name,age) VALUES ('duktig',23);
```

3、事务B提交后，事务A执行select操作，查询`age=23`的数据，这时出现了多一行的数据，这是事务B刚刚插入的，即幻读。

![可重复读分析](https://gitee.com/koala010/typora/raw/master/img/20210627190808.png)

看到有文章提到，在Mysql中，默认的不可重复读个隔离级别也解决了幻读的问题。但是我这确实出现了幻读问题，这需要再分析分析。



##### 串行化

串行化是4种事务隔离级别中隔离效果最好的，解决了脏读、可重复读、幻读的问题，但是效果最差，它将事务的执行变为顺序执行，与其他三个隔离级别相比，它就相当于单线程，后一个事务的执行必须等待前一个事务结束。

### 3.6 MySQL 中是如何实现事务隔离的？

首先说读未提交，它是性能最好，也可以说它是最野蛮的方式，因为它压根儿就不加锁，所以根本谈不上什么隔离效果，可以理解为没有隔离。

再来说串行化。读的时候加共享锁，也就是其他事务可以并发读，但是不能写。写的时候加排它锁，其他事务不能并发写也不能并发读。

最后说读提交和可重复读。这两种隔离级别是比较复杂的，既要允许一定的并发，又想要兼顾的解决问题。

#### 实现可重复读

为了解决不可重复读，或者为了实现可重复读，MySQL 采用了 MVVC (多版本并发控制) 的方式。

我们**在数据库表中看到的一行记录可能实际上有多个版本**，每个版本的记录除了有数据本身外，还要有一个表示版本的字段，记为`row trx_id`，而这个字段就是使其产生的事务的id，事务id记为 `transaction id`，它在事务开始的时候向事务系统申请，按时间先后顺序递增。

![可重复读实现原理图](https://gitee.com/koala010/typora/raw/master/img/20210627191801.png)

按照上面这张图理解，一行记录现在有 3 个版本，每一个版本都记录这使其产生的事务 ID，比如事务A的transaction id 是100，那么版本1的row trx_id 就是 100，同理版本2和版本3。

快照，学名叫做一致性视图。可重复读是在事务开始的时候生成一个当前事务全局性的快照，而读提交则是每次执行语句的时候都重新生成一次快照。

对于一个快照来说，它能够读到那些版本数据，要遵循以下规则：

1. 当前事务内的更新，可以读到；
2. 版本未提交，不能读到；
3. 版本已提交，但是却在快照创建后提交的，不能读到；
4. 版本已提交，且是在快照创建前提交的，可以读到；

利用上面的规则，再返回去套用到读提交和可重复读的那两张图上就很清晰了。两者主要的区别就是在快照的创建上，可重复读仅在事务开始是创建一次，而读提交每次执行语句的时候都要重新创建一次。

大致理解为：读提交只有在事务Acommit后事务B才能独到其数据，所以解决了脏读问题。可重复读只能在事务开始时才能读到数据，所以无论在事务的那个阶段，读到的数据都是一致的。

#### 并发写问题

存在这的情况，两个事务，对同一条数据做修改。最后结果应该是哪个事务的结果呢，肯定要是时间靠后的那个对不对。并且更新之前要先读数据，这里所说的读和上面说到的读不一样，更新之前的读叫做“当前读”，总是当前版本的数据，也就是多版本中最新一次提交的那版。

假设事务A执行 update 操作， update 的时候要对所修改的行加行锁，这个行锁会在提交之后才释放。而在事务A提交之前，事务B也想 update 这行数据，于是申请行锁，但是由于已经被事务A占有，事务B是申请不到的，此时，事务B就会一直处于等待状态，直到事务A提交，事务B才能继续执行，如果事务A的时间太长，那么事务B很有可能出现超时异常。

加锁的过程要分有索引和无索引两种情况，比如下面这条语句

```mysql
update user set age=11 where id = 1
```

id 是这张表的主键，是有索引的情况，那么 MySQL 直接就在索引数中找到了这行数据，然后干净利落的加上行锁就可以了。

而下面这条语句

```mysql
update user set age=11 where age=10
```

表中并没有为 age 字段设置索引，所以， MySQL 无法直接定位到这行数据。那怎么办呢，当然也不是加表锁了。MySQL 会为这张表中所有行加行锁，没错，是所有行。但是呢，在加上行锁后，MySQL 会进行一遍过滤，发现不满足的行就释放锁，最终只留下符合条件的行。虽然最终只为符合条件的行加了锁，但是这一锁一释放的过程对性能也是影响极大的。所以，如果是大表的话，建议合理设计索引，如果真的出现这种情况，那很难保证并发度。

#### 幻读问题

上面介绍可重复读的时候，那张图里标示着出现幻读的地方实际上在 MySQL 中并不会出现，MySQL 已经在可重复读隔离级别下解决了幻读的问题。

前面刚说了并发写问题的解决方式就是行锁，而解决幻读用的也是锁，叫做间隙锁，MySQL 把行锁和间隙锁合并在一起，解决了并发写和幻读的问题，这个锁叫做 Next-Key锁。

假设现在表中有两条记录，并且 age 字段已经添加了索引，两条记录 age 的值分别为 10 和 30。

此时，在数据库中会为索引维护一套B+树，用来快速定位行记录。B+索引树是有序的，所以会把这张表的索引分割成几个区间。

![幻读问题解决](https://gitee.com/koala010/typora/raw/master/img/20210627192740.png)

如图所示，分成了3 个区间，(负无穷,10]、(10,30]、(30,正无穷]，在这3个区间是可以加间隙锁的。

在事务A提交之前，事务B的插入操作只能等待，这就是间隙锁起得作用。在事务A执行更新操作时，

`update user set name='风筝2号’ where age = 10;` 的时候，由于条件 where age = 10 ，数据库不仅在 age =10 的行上添加了行锁，而且在这条记录的两边，也就是(负无穷,10]、(10,30]这两个区间加了间隙锁，从而导致事务B插入操作无法完成，只能等待事务A提交。不仅插入 age = 10 的记录需要等待事务A提交，age<10、10<age<30 的记录页无法完成，而大于等于30的记录则不受影响，这足以解决幻读问题了。

这是有索引的情况，如果 age 不是索引列，那么数据库会为整个表加上间隙锁。所以，如果是没有索引的话，不管 age 是否大于等于30，都要等待事务A提交才可以成功插入。

参考：

- [我以为我对Mysql事务很熟，直到我遇到了阿里面试官](https://zhuanlan.zhihu.com/p/148035779)
- [MySQL事务隔离级别和实现原理（看这一篇文章就够了！）](https://zhuanlan.zhihu.com/p/117476959)
- [面试官:谈谈你对mysql事务的认识?](https://zhuanlan.zhihu.com/p/166158027)
- [MySQL 聊聊MySQL锁机制及事务](https://blog.csdn.net/cong____cong/article/details/106382923?spm=1001.2014.3001.5501)



## 4. 主从复制和读写分离

### 4.1 为什么使用主从复制和读写分离？

主从复制、读写分离一般是一起使用的。目的很简单，就是**为了提高数据库的并发性能**。

假设是单机，读写都在一台MySQL上面完成，性能肯定不高。如果有三台MySQL，一台mater只负责写操作，两台salve只负责读操作，性能不就能大大提高了。

随着业务量的扩展、如果是单机部署的MySQL，会导致I/O频率过高。采用**主从复制、读写分离可以提高数据库的可用性**。

### 4.2 主从复制的原理

①当Master节点进行`insert`、`update`、`delete`操作时，会按顺序写入到binlog中。

②salve从库连接master主库，Master有多少个slave就会创建多少个binlog dump线程。

③当Master节点的binlog发生变化时，binlog dump 线程会通知所有的salve节点，并将相应的binlog内容推送给slave节点。

④I/O线程接收到 binlog 内容后，将内容写入到本地的 relay-log。

⑤SQL线程读取I/O线程写入的relay-log，并且根据 relay-log 的内容对从数据库做对应的操作。

![主从复制流程](https://gitee.com/koala010/typora/raw/master/img/20210702153628.png)

### 4.3 如何实现主从复制？

用三台虚拟机(Linux)演示，IP分别是104(Master)，106(Slave)，107(Slave)，预期效果如下图所示

![主从复制效果](https://gitee.com/koala010/typora/raw/master/img/20210702155210.png)

#### Master配置

1、配置用户允许从节点的IP进行访问

```sql
//192.168.0.106是slave从机的IP
GRANT REPLICATION SLAVE ON *.* to 'root'@'192.168.0.106' identified by 'Java@1234';
//192.168.0.107是slave从机的IP
GRANT REPLICATION SLAVE ON *.* to 'root'@'192.168.0.107' identified by 'Java@1234';
//刷新系统权限表的配置
FLUSH PRIVILEGES;
```

2、开启binlog，配置需要同步的数据库（不配置则同步全部数据库）和binlog日志的保留天数（过大可能导致磁盘空间不足）

找到mysql的配置文件`/etc/my.cnf`，增加以下配置：

```sql
# 开启binlog
log-bin=mysql-bin
server-id=104
# 需要同步的数据库，如果不配置则同步全部数据库
binlog-do-db=test_db
# binlog日志保留的天数，清除超过10天的日志
# 防止日志文件过大，导致磁盘空间不足
expire-logs-days=10 
```

配置完成后，重启mysql：

```text
service mysql restart
```

可以通过命令行`show master status\G;`查看当前binlog日志的信息(后面有用)：

![image-20210702155340610](https://gitee.com/koala010/typora/raw/master/img/20210702155340.png)

#### Slave配置

`/etc/my.cnf`配置文件，增加以下配置：

```sql
# 不要和其他mysql服务id重复即可
server-id=106
```

登录后进行相关命令配置：

```sql
CHANGE MASTER TO 
MASTER_HOST='192.168.0.104',//主机IP
MASTER_USER='root',//之前创建的用户账号
MASTER_PASSWORD='Java@1234',//之前创建的用户密码
MASTER_LOG_FILE='mysql-bin.000001',//master主机的binlog日志名称
MASTER_LOG_POS=862,//binlog日志偏移量
master_port=3306;//端口
```

设置完之后需要启动：

```sql
# 启动slave服务
start slave;
```

启动完之后怎么校验是否启动成功呢？使用以下命令：

```text
show slave status\G;
```

可以看到如下信息（摘取部分关键信息）：

```sql
*************************** 1. row ***************************
               Slave_IO_State: Waiting for master to send event
                  Master_Host: 192.168.0.104
                  Master_User: root
                  Master_Port: 3306
                Connect_Retry: 60
              Master_Log_File: mysql-bin.000001
          Read_Master_Log_Pos: 619
               Relay_Log_File: mysqld-relay-bin.000001
                Relay_Log_Pos: 782
        Relay_Master_Log_File: mysql-bin.000001 //binlog日志文件名称
             Slave_IO_Running: Yes //Slave_IO线程、SQL线程都在运行
            Slave_SQL_Running: Yes
             Master_Server_Id: 104 //master主机的服务id
                  Master_UUID: 0ab6b3a6-e21d-11ea-aaa3-080027f8d623
             Master_Info_File: /var/lib/mysql/master.info
                    SQL_Delay: 0
          SQL_Remaining_Delay: NULL
      Slave_SQL_Running_State: Slave has read all relay log; waiting for the slave I/O thread to update it
           Master_Retry_Count: 86400
                Auto_Position: 0
```

另一台slave从机配置一样，不再赘述。

#### 测试主从复制

在master主机执行sql：

```sql
CREATE TABLE `tb_commodity_info` (
  `id` varchar(32) NOT NULL,
  `commodity_name` varchar(512) DEFAULT NULL COMMENT '商品名称',
  `commodity_price` varchar(36) DEFAULT '0' COMMENT '商品价格',
  `number` int(10) DEFAULT '0' COMMENT '商品数量',
  `description` varchar(2048) DEFAULT '' COMMENT '商品描述',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='商品信息表';
```

接着我们可以看到两台slave从机同步也创建了商品信息表，主从复制就完成了！



### 4.4 如何实现读写分离？

主从复制完成后，我们还需要实现读写分离，master负责写入数据，两台slave负责读取数据。

使用ShardingSphere-JDBC

#### 缺点

尽管主从复制、读写分离能很大程度保证MySQL服务的高可用和提高整体性能，但是问题也不少：

- **从机是通过binlog日志从master同步数据的，如果在网络延迟的情况，从机就会出现数据延迟。那么就有可能出现master写入数据后，slave读取数据不一定能马上读出来**。

可能有人会问，有没有事务问题呢？

实际上这个框架已经想到了，我们看回之前的那个截图，有一句话是这样的：

![image-20210702161523262](https://gitee.com/koala010/typora/raw/master/img/20210702161523.png)

参看：[MySQL主从复制读写分离，看这篇就够了！](https://zhuanlan.zhihu.com/p/199217698)

### 4.5 如何解决 MySQL 主从同步的延时和数据丢失问题？

**问题1：主从同步延时**

这里有一个非常重要的一点，就是从库同步主库数据的过程是串行化的，也就是说主库上并行的操作，在从库上会串行执行。所以这就是一个非常重要的点了，由于从库从主库拷贝日志以及串行执行 SQL 的特点，在高并发场景下，从库的数据一定会比主库慢一些，是**有延时**的。所以经常出现，刚写入主库的数据可能是读不到的，要过几十毫秒，甚至几百毫秒才能读取到。

解决：**并行复制**

> **并行复制**，指的是从库开启多个线程，并行读取 relay log 中不同库的日志，然后**并行重放不同库的日志**，这是库级别的并行。

**问题2：主库宕机，数据丢失**

如果主库突然宕机，然后恰好数据还没同步到从库，那么有些数据可能在从库上是没有的，有些数据可能就丢失了。

解决：**半同步复制**

> **半同步复制**，也叫`semi-sync`复制，指的就是主库写入 binlog 日志之后，就会将**强制**此时立即将数据同步到从库，从库将日志写入自己本地的 relay log 之后，接着会返回一个 ack 给主库，主库接收到**至少一个从库**的 ack 之后才会认为写操作完成了。

**MySQL 主从同步延时问题（精华）**

以前线上确实处理过因为主从同步延时问题而导致的线上的 bug，属于小型的生产事故。

是这个么场景。有个同学是这样写代码逻辑的。先插入一条数据，再把它查出来，然后更新这条数据。在生产环境高峰期，写并发达到了 2000/s，这个时候，主从复制延时大概是在小几十毫秒。线上会发现，每天总有那么一些数据，我们期望更新一些重要的数据状态，但在高峰期时候却没更新。用户跟客服反馈，而客服就会反馈给我们。


我们通过 MySQL 命令：

```text
show status
```

查看 `Seconds_Behind_Master`，可以看到从库复制主库的数据落后了几 ms。

一般来说，如果主从延迟较为严重，有以下解决方案：

- 分库，将一个主库拆分为多个主库，每个主库的写并发就减少了几倍，此时主从延迟可以忽略不计。
- 打开 MySQL 支持的并行复制，多个库并行复制。如果说某个库的写入并发就是特别高，单库写并发达到了 2000/s，并行复制还是没意义。
- 重写代码，写代码的同学，要慎重，插入数据时立马查询可能查不到。
- 如果确实是存在必须先插入，立马要求就查询到，然后立马就要反过来执行一些操作，对这个查询**设置直连主库**。**不推荐**这种方法，你这么搞导致读写分离的意义就丧失了。

参考：[如何实现 MySQL 的读写分离？如何解决 MySQL 主从同步的延时问题？](https://zhuanlan.zhihu.com/p/60455737)

## 5. 分表分库

### 5.1 数据库的瓶颈问题

不管是IO瓶颈，还是CPU瓶颈，最终都会导致数据库的活跃连接数增加，进而逼近甚至达到数据库可承载活跃连接数的阈值。在业务Service来看就是，可用数据库连接少甚至无连接可用。接下来就可以想象了吧（并发量、吞吐量、崩溃）。

#### IO瓶颈

第一种：磁盘读IO瓶颈，热点数据太多，数据库缓存放不下，每次查询时会产生大量的IO，降低查询速度 -> **分库和垂直分表**。

第二种：网络IO瓶颈，请求的数据太多，网络带宽不够 -> **分库**。

#### CPU瓶颈

第一种：SQL问题，如SQL中包含join，group by，order by，非索引字段条件查询等，增加CPU运算的操作 -> SQL优化，建立合适的索引，在业务Service层进行业务计算。

第二种：单表数据量太大，查询时扫描的行太多，SQL效率低，CPU率先出现瓶颈 -> **水平分表**。

### 5.2 水平分表原理

以**字段**为依据，按照一定策略（hash、range等），将一个**表**中的数据拆分到多个**表**中。

结果：

- 每个**表**的**结构**都一样；
- 每个**表**的**数据**都不一样，没有交集；
- 所有**表**的**并集**是全量数据；

使用场景：系统绝对并发量并没有上来，只是单表的数据量太多，影响了SQL效率，加重了CPU负担，以至于成为瓶颈。

分析：表的数据量少了，单次SQL执行效率高，自然减轻了CPU的负担。

### 5.3 水平分库原理

以**字段**为依据，按照一定策略（hash、range等），将一个**库**中的数据拆分到多个**库**中。

结果：

- 每个**库**的**结构**都一样；
- 每个**库**的**数据**都不一样，没有交集；
- 所有**库**的**并集**是全量数据；

使用场景：系统绝对并发量上来了，分表难以根本上解决问题，并且还没有明显的业务归属来垂直分库。

分析：库多了，io和cpu的压力自然可以成倍缓解。

### 5.4 垂直分表原理

以**字段**为依据，按照字段的活跃性，将**表**中字段拆到不同的**表**（主表和扩展表）中。

结果：

- 每个**表**的**结构**都不一样；
- 每个**表**的**数据**也不一样，一般来说，每个表的**字段**至少有一列交集，一般是主键，用于关联数据；
- 所有**表**的**并集**是全量数据；

使用场景：系统绝对并发量并没有上来，表的记录并不多，但是字段多，并且热点数据和非热点数据在一起，单行数据所需的存储空间较大。以至于数据库缓存的数据行减少，查询时会去读磁盘数据产生大量的随机读IO，产生IO瓶颈。

分析：

可以用列表页和详情页来帮助理解。垂直分表的拆分原则是将热点数据（可能会冗余经常一起查询的数据）放在一起作为主表，非热点数据放在一起作为扩展表。这样更多的热点数据就能被缓存下来，进而减少了随机读IO。

但记住，千万别用join，因为join不仅会增加CPU负担并且会讲两个表耦合在一起（必须在一个数据库实例上）。关联数据，应该在业务Service层做文章，分别获取主表和扩展表数据然后用关联字段关联得到全部数据。

### 5.5 垂直分库原理

以**表**为依据，按照业务归属不同，将不同的**表**拆分到不同的**库**中。

结果：

- 每个**库**的**结构**都不一样；
- 每个**库**的**数据**也不一样，没有交集；
- 所有**库**的**并集**是全量数据；

使用场景：系统绝对并发量上来了，并且可以抽象出单独的业务模块。

分析：到这一步，基本上就可以服务化了。例如，随着业务的发展一些公用的配置表、字典表等越来越多，这时可以将这些表拆到单独的库中，甚至可以服务化。再有，随着业务的发展孵化出了一套业务模式，这时可以将相关的表拆到单独的库中，甚至可以服务化。

### 5.6 分表分库的步骤

根据容量（当前容量和增长量）评估分库或分表个数 -> 选key（均匀）-> 分表规则（hash或range等）-> 执行（一般双写）-> 扩容问题（尽量减少数据的移动）。

### 5.7 水平分库/分表方法

#### RANGE

指定一个数据范围来进行分表，例如从1~1000000，1000001-2000000，使用一百万一张表的方式，如下图所示：

![水平分表分库-Range法](https://gitee.com/koala010/typora/raw/master/img/20210702171732.png)

这种方法需要维护表的ID，特别是分布式环境下，这种分布式ID,在不使用第三方分表工具的情况下，建议使用Redis，Redis的`incr`操作可以轻松的维护分布式的表ID。

> Redis Incr 命令将 key 中储存的数字值增一。
>
> 如果 key 不存在，那么 key 的值会先被初始化为 0 ，然后再执行 INCR 操作。

**优点**： 扩容简单，提前建好库、表就好

**缺点**：**大部分读和写都访会问新的数据，有IO瓶颈，这样子造成新库压力过大，不建议采用**。

#### Hash取模

针对RANGE方式分表有IO瓶颈的问题，可以采用**根据用户ID HASG取模**的方式进行分库分表，如图所示：

<img src="https://gitee.com/koala010/typora/raw/master/img/20210702172048.png" alt="水平分表分库-Hash取模" style="zoom: 67%;" />

这样就可以将数据分散在不同的库、表中，避免了IO瓶颈的问题。

**优点：** 能保证数据较均匀的分散落在不同的库、表中，减轻了数据库压力。

**缺点：** **扩容麻烦、迁移数据时每次都需要重新计算hash值分配到不同的库和表**。

#### 一致性Hash

**普通HASH算法：**

普通哈希算法将任意长度的二进制值映射为较短的固定长度的二进制值，这个小的二进制值称为哈希值。哈希值是一段数据唯一且极其紧凑的数值表示形式。

普通的hash算法在分布式应用中的不足：在分布式的存储系统中，要将数据存储到具体的节点上，如果我们采用普通的hash算法进行路由，将数据映射到具体的节点上，如key%n，key是数据的key，n是机器节点数，如果有一个机器加入或退出集群，则所有的数据映射都无效了，如果是持久化存储则要做数据迁移，如果是分布式缓存，则其他缓存就失效了。

**一致性HASH算法：**

按照常用的hash算法来将对应的key哈希到一个具有2^32次方个节点的空间中，即0~ (2^32)-1的数字空间中。现在我们可以将这些数字头尾相连，想象成一个闭合的环形。

**原理参看HaashMap的一致性哈希**。

优点：通过虚拟节点方式能保证数据较均匀的分散落在不同的库、表中，并且新增、删除节点不影响其他节点的数据，高可用、容灾性强。

参考：

- [MySQL分库分表 看完吊打面试官](https://zhuanlan.zhihu.com/p/348659067)
- [MySQL：互联网公司常用分库分表方案汇总！](https://zhuanlan.zhihu.com/p/137368446)

## 6. 数据库优化

[MySQL 对于千万级的大表要怎么优化？](https://www.zhihu.com/question/19719997/answer/81930332)

### 6.1 mysql 单表多次查询和多表联合查询，哪个效率高?

![image-20210726210904393](https://gitee.com/koala010/typora/raw/master/img/20210726210911.png)



单表查询有利于后期数据量大了分库分表，如果联合查询的话，一旦分库，原来的sql都需要改动









