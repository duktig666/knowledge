# 数据结构设计

数据结构设计题主要就是给你提需求，让你实现 API，⽽且要求这些 API 的复杂度尽可能低。

根据经验，设计题中 **哈希表的出现频率很⾼**，⼀般都是各类其他数据结构和哈希表组合，从⽽改善这些基本数据结构的特性，获得「超能⼒」。

## LRU缓存机制

LRU 算法就是⼀种缓存淘汰策略，原理不难，但是⾯试中写出没有 bug 的算法⽐较有技巧，需要对数据结构进⾏层层抽象和拆解。

计算机的缓存容量有限，如果缓存满了就要删除⼀些内容，给新内容腾位置。但问题是，删除哪些内容呢？

我们肯定希望删掉哪些没什么⽤的缓存，⽽把有⽤的数据继续留在缓存⾥，⽅便之后继续使⽤。那么，**什么样的数据，我们判定为「有⽤的」的数据呢**？

LRU 缓存淘汰算法就是⼀种常⽤策略。LRU 的全称是 Least Recently Used，也就是说我们认为 **最近使⽤过的数据应该是是「有⽤的」**，很久都没⽤过的数据应该是⽆⽤的，内存满了就优先删那些很久没⽤过的数据。

场景：比如手机杀后台的策略。

> [146. LRU 缓存机制](https://leetcode-cn.com/problems/lru-cache/)

⾸先要接收⼀个 capacity 参数作为缓存的最⼤容量，然后实现两个 API，⼀个是 put(key, val) ⽅法存⼊键值对，另⼀个是 get(key) ⽅法获取 key 对应的 val，如果 key 不存在则返回 -1。

要让 put 和 get ⽅法的时间复杂度为 O(1)，我们可以总结出 cache 这个数据结构必
要的条件：

1. 显然 cache 中的元素必须有时序，以区分最近使⽤的和久未使⽤的数据，当容量满了之后要删除最久未使⽤的那个元素腾位置。
2. 我们要在 cache 中快速找某个 key 是否已存在并得到对应的 val。
3. 每次访问 cache 中的某个 key，需要将这个元素变为最近使⽤的，也就是说 cache 要⽀持在任意位置快速插⼊和删除元素。

那么，什么数据结构同时符合上述条件呢？哈希表查找快，但是数据⽆固定顺序；链表有顺序之分，插⼊删除快，但是查找慢。所以结合⼀下，形成⼀种新的数据结构：哈希链表 `LinkedHashMap`。

![哈希链表](https://cos.duktig.cn/typora/202112021532046.png)

借助这个结构，我们来逐⼀分析上⾯的 3 个条件：

1. 如果我们每次默认从链表尾部添加元素，那么显然越靠尾部的元素就是最近使⽤的，越靠头部的元素就是最久未使⽤的。
2. 对于某⼀个 key，我们可以通过哈希表快速定位到链表中的节点，从⽽取得对应 val。
3. 链表显然是⽀持在任意位置快速插⼊和删除的，改改指针就⾏。只不过传统的链表⽆法按照索引快速访问某⼀个位置的元素，⽽这⾥借助哈希表，可以通过 key 快速映射到任意⼀个链表节点，然后进⾏插⼊和删除。

**为什么必须要⽤双向链表？**

因为我们需要删除操作。删除⼀个节点不光要得到该节点本身的指针，也需要操作其前驱节点的指针，⽽双向链表才能⽀持直接查找前驱，保证操作的时间复杂度 O(1)。

**为什么要在链表中同时存储 key 和 val，⽽不是只存储 val？**

`removeLeastRecently` 函数中，我们需要⽤ `deletedNode` 得到 `deletedKey`。

也就是说，当缓存容量已满，我们不仅仅要删除最后⼀个 Node 节点，还要把 map 中映射到该节点的 key 同时删除，⽽这个 key 只能由 Node 得到。如果 Node 结构中只存储 val，那么我们就⽆法得知 key 是什么，就⽆法删除 map 中的键，造成错误。

put ⽅法稍微复杂⼀些，我们先来画个图搞清楚它的逻辑：

![image-20211202165137688](https://cos.duktig.cn/typora/202112021651029.png)

### 借助LinkedHashMap实现

```java
public class LRUCacheAPI {

    int capacity;

    LinkedHashMap<Integer, Integer> cache = new LinkedHashMap<>();

    public LRUCacheAPI(int capacity) {
        this.capacity = capacity;
    }

    public int get(int key) {
        if (! cache.containsKey(key)) {
            return - 1;
        }
        // 将 key 变为最近使⽤
        makeRecently(key);
        return cache.get(key);
    }

    public void put(int key, int val) {
        if (cache.containsKey(key)) {
            // 修改 key 的值
            cache.put(key, val);
            // 将 key 变为最近使⽤
            makeRecently(key);
            return;
        }

        if (cache.size() >= this.capacity) {
            // 链表头部就是最久未使⽤的 key
            int oldestKey = cache.keySet().iterator().next();
            cache.remove(oldestKey);
        }
        // 将新的 key 添加链表尾部
        cache.put(key, val);
    }

    private void makeRecently(int key) {
        int val = cache.get(key);
        // 删除 key，重新插⼊到队尾
        cache.remove(key);
        cache.put(key, val);
    }

}
```



### 简化版自定义代码

```java
public class LRUCache {

    private static class DoubleLinkedNode {
        int k;
        int v;
        DoubleLinkedNode prev;
        DoubleLinkedNode next;

        DoubleLinkedNode() {}

        DoubleLinkedNode(int k, int v) {
            this.k = k;
            this.v = v;
        }
    }

    private Map<Object, DoubleLinkedNode> cache = new HashMap<>();
    private int size;
    private int capacity;
    private DoubleLinkedNode head, tail;

    public LRUCache(int capacity) {
        this.size = 0;
        this.capacity = capacity;
        // 使用伪头部和伪尾部节点
        head = new DoubleLinkedNode();
        tail = new DoubleLinkedNode();
        head.next = tail;
        tail.prev = head;
    }

    public int get(int key) {
        DoubleLinkedNode node = cache.get(key);
        if (node == null) {
            return - 1;
        }
        this.moveToHead(node);
        return node.v;
    }

    public void put(int key, int value) {
        DoubleLinkedNode node = cache.get(key);
        if (node == null) {
            // 如果没有当前元素，新创建
            DoubleLinkedNode newNode = new DoubleLinkedNode(key, value);
            cache.put(key, newNode);
            this.addToHead(newNode);
            ++ size;
            if (size > capacity) {
                DoubleLinkedNode tail = this.removeTail();
                // 删除哈希表中对应的项
                cache.remove(tail.k);
                -- size;
            }
        } else {
            node.v = value;
            moveToHead(node);
        }

    }

    private DoubleLinkedNode removeTail() {
        DoubleLinkedNode res = tail.prev;
        this.removeNode(res);
        return res;
    }


    private void moveToHead(DoubleLinkedNode node) {
        this.removeNode(node);
        this.addToHead(node);
    }

    private void addToHead(DoubleLinkedNode node) {
        node.prev = head;
        node.next = head.next;
        head.next.prev = node;
        head.next = node;
    }

    private void removeNode(DoubleLinkedNode node) {
        node.prev.next = node.next;
        node.next.prev = node.prev;
    }


}
```



### 完善的自定义代码

完整代码如下：

```java
public class LRUCache<K, V> {

    /** key -> Node(key, val) */
    private Map<K, Node<K, V>> map;

    /** Node(k1, v1) <-> Node(k2, v2)... */
    private DoubleList<K, V> cache;

    /** 最大容量 */
    private int capacity;

    public LRUCache(int capacity) {
        this.capacity = capacity;
        map = new HashMap<>();
        cache = new DoubleList<>();
    }

    /**
     * 将某个 key 提升为最近使⽤的
     */
    private void makeRecently(K key) {
        Node<K, V> x = map.get(key);
        // 先从链表中删除这个节点
        cache.remove(x);
        // 重新插到队尾
        cache.addLast(x);
    }

    /**
     * 添加最近使⽤的元素
     */
    private void addRecently(K key, V val) {
        Node<K, V> node = new Node<>(key, val);
        // 链表尾部就是最近使⽤的元素
        cache.addLast(node);
        // 别忘了在 map 中添加 key 的映射
        map.put(key, node);
    }

    /**
     * 删除某⼀个 key
     */
    private void deleteKey(K key) {
        Node<K, V> node = map.get(key);
        // 从链表中删除
        cache.remove(node);
        // 从 map 中删除
        map.remove(key);
    }

    /**
     * 删除最久未使⽤的元素
     */
    private void removeLeastRecently() {
        // 链表头部的第⼀个元素就是最久未使⽤的
        Node<K, V> deletedNode = cache.removeFirst();
        // 同时别忘了从 map 中删除它的 key
        K deletedKey = deletedNode.key;
        map.remove(deletedKey);
    }

    /**
     * 从缓存中取值，并将当前的值设为最近使用过的
     */
    public V get(K key) {
        if (! map.containsKey(key)) {
            return null;
        }
        // 将该数据提升为最近使⽤的
        this.makeRecently(key);
        return map.get(key).val;
    }

    /**
     * 往缓存添加数据
     * 如果存在删除旧数据，将新数据添加到链表尾部
     * 如果达到容量，删除头部的元素，新元素添加到尾部
     */
    public void put(K key, V val) {
        if (map.containsKey(key)) {
            // 删除旧的数据
            deleteKey(key);
            // 新插⼊的数据为最近使⽤的数据
            addRecently(key, val);
            return;
        }

        if (capacity == cache.size()) {
            // 删除最久未使⽤的元素
            removeLeastRecently();
        }
        // 添加为最近使⽤的元素
        addRecently(key, val);
    }

    /**
     * 双向链表的节点
     */
    private static class Node<K, V> {
        K key;
        V val;
        Node<K, V> next, prev;

        public Node(K k, V v) {
            this.key = k;
            this.val = v;
        }
    }

    /**
     * 双向链表
     */
    static class DoubleList<K, V> {
        // 头尾虚节点
        private Node<K, V> head, tail;
        // 链表元素数
        private int size;

        /**
         * 初始化双向链表的数据
         */
        public DoubleList() {
            head = new Node<>(null, null);
            tail = new Node<>(null, null);
            head.next = tail;
            tail.prev = head;
            size = 0;
        }

        /**
         * 在链表尾部添加节点 node，时间 O(1)
         */
        public void addLast(Node<K, V> node) {
            node.prev = tail.prev;
            node.next = tail;
            tail.prev.next = node;
            tail.prev = node;
            size++;
        }

        /**
         * 删除链表中的 node 节点（node ⼀定存在）
         * 由于是双链表且给的是⽬标 Node 节点，时间 O(1)
         */
        public void remove(Node<K, V> node) {
            node.prev.next = node.next;
            node.next.prev = node.prev;
            size--;
        }

        /**
         * 删除链表中第⼀个节点，并返回该节点，时间 O(1)
         *
         * @return del node
         */
        public Node<K, V> removeFirst() {
            // 如果只有一个节点，直接返回null
            if (head.next == tail) {
                return null;
            }
            Node<K, V> first = head.next;
            this.remove(first);
            return first;
        }

        /**
         * 返回链表⻓度，时间 O(1)
         */
        public int size() { return size; }

    }
    
}
```

代码步骤：

- 构造双向链表的节点
- 构造双向链表
  - 初始化数据，尤其是虚拟头结点和尾节点，以及链表的长度
  - 提供双向链表常用的API
    - 在链表尾部添加节点
    - 删除链表中的指定节点
    - 删除链表中的第一个节点，并返回该节点
    - 返回链表的长度
- 构造LRU缓存类
  - 初始化 map、双向链表和缓存容量
  - 提供封装的私有方法，同时操作缓存（双向链表）和map可能出现失误
    - 将某个可以提升为最近使用的。删除节点，重新插入到尾部。
    - 添加最近使用的元素。节点添加到尾部，并加入到map的映射中。
    - 删除一个key。删除链表节点，同时删除map的key。
    - 删除最久未使用的元素。删除链表节点，得到key，根据key删除map的映射。
  - 提供get方法，获取缓存的值。
    - 不存在，返回null
    - 将该数据提升为最近使用的
    - 返回该数据
  - 提供put方法，修改和新增缓存的值
    - 存在key，删除旧数据，新数据插入到头部
    - 达到缓存容量，删除最久未使用的数据，将新数据添加到头部

## LFU缓存机制

LRU 算法的淘汰策略是 Least Recently Used，也就是每次淘汰那些最久没被使⽤的数据；

LFU 算法的淘汰策略是 Least Frequently Used，也就是 **每次淘汰那些使⽤次数最少的数据**。

LRU 算法的核⼼数据结构是使⽤哈希链表 `LinkedHashMap`，⾸先借助链表的有序性使得链表元素维持插⼊顺序，同时借助哈希映射的快速访问能⼒使得我们可以在 O(1) 时间访问链表的任意元素。

从实现难度上来说，LFU 算法的难度⼤于 LRU 算法，因为 LRU 算法相当于把数据按照时间排序，这个需求借助链表很⾃然就能实现，你⼀直从链表头部加⼊元素的话，越靠近头部的元素就是新的数据，越靠近尾部的元素就是旧的数据，我们进⾏缓存淘汰的时候只要简单地将尾部的元素淘汰掉就⾏了。

⽽ LFU 算法相当于是把数据按照访问频次进⾏排序，这个需求恐怕没有那么简单，⽽且还有⼀种情况，如果多个数据拥有相同的访问频次，我们就得删除最早插⼊的那个数据。也就是说 **LFU 算法是淘汰访问频次最低的数据，如果访问频次最低的数据有多条，需要淘汰最旧的数据**。

**算法描述**：

> 要求你写⼀个类，接受⼀个 capacity 参数，实现 get 和 put ⽅法：
>
> get(key) ⽅法会去缓存中查询键 key，如果 key 存在，则返回 key 对应的 val，否则返回 -1。
>
> put(key, value) ⽅法插⼊或修改缓存。如果 key 已存在，则将它对应的值改为 val；如果 key 不存在，则插⼊键值对 (key, val)。
>
> 当缓存达到容量 capacity 时，则应该在插⼊新的键值对之前，删除使⽤频次（后⽂⽤ freq 表示）最低的键值对。如果 freq 最低的键值对有多个，则删除其中最旧的那个。

**思路分析：**

需求：

1、调用`get(key)`方法时，要返回该`key`对应的`val`。

2、只要用`get`或者`put`方法访问一次某个`key`，该`key`的`freq`就要加一。

3、如果在容量满了的时候进行插入，则需要将`freq`最小的`key`删除，如果最小的`freq`对应多个`key`，则删除其中最旧的那一个。

好的，我们希望能够在 O(1) 的时间内解决这些需求，可以使⽤基本数据结构来逐个击破：

**1、**使用一个`HashMap`存储`key`到`val`的映射，就可以快速计算`get(key)`。

```
HashMap<Integer, Integer> keyToVal;
```

**2、**使用一个`HashMap`存储`key`到`freq`的映射，就可以快速操作`key`对应的`freq`。

```
HashMap<Integer, Integer> keyToFreq;
```

**3、**这个需求应该是 LFU 算法的核心，所以我们分开说。

**3.1****、**首先，肯定是需要`freq`到`key`的映射，用来找到`freq`最小的`key`。

**3.2、**将`freq`最小的`key`删除，那你就得快速得到当前所有`key`最小的`freq`是多少。想要时间复杂度 O(1) 的话，肯定不能遍历一遍去找，那就用一个变量`minFreq`来记录当前最小的`freq`吧。

**3.3、**可能有多个`key`拥有相同的`freq`，所以 **`freq`对`key`是一对多的关系**，即一个`freq`对应一个`key`的列表。

**3.4、**希望`freq`对应的`key`的列表是**存在时序**的，便于快速查找并删除最旧的`key`。

**3.5、**希望**能够快速删除`key`列表中的任何一个`key`**，因为如果频次为`freq`的某个`key`被访问，那么它的频次就会变成`freq+1`，就应该从`freq`对应的`key`列表中删除，加到`freq+1`对应的`key`的列表中。

介绍⼀下这个`LinkedHashSet`，它满⾜我们 3.3，3.4，3.5 这⼏个要求。你会发现普通的链表`LinkedList` 能够满⾜ 3.3，3.4 这两个要求，但是由于普通链表不能快速访问链表中的某⼀个节点，所以⽆法满⾜ 3.5 的要求。

`LinkedHashSet` 顾名思义，是链表和哈希集合的结合体。链表不能快速访问链表节点，但是插⼊元素具有时序；哈希集合中的元素⽆序，但是可以对元素进⾏快速的访问和删除。

那么，它俩结合起来就兼具了哈希集合和链表的特性，既可以在 O(1) 时间内访问或删除其中的元素，⼜可以保持插⼊的时序，⾼效实现 3.5 这个需求。

基本数据结构如下：

```java
class LFUCache {
    // key 到 val 的映射，我们后文称为 KV 表
    HashMap<Integer, Integer> keyToVal;
    // key 到 freq 的映射，我们后文称为 KF 表
    HashMap<Integer, Integer> keyToFreq;
    // freq 到 key 列表的映射，我们后文称为 FK 表
    HashMap<Integer, LinkedHashSet<Integer>> freqToKeys;
    // 记录最小的频次
    int minFreq;
    // 记录 LFU 缓存的最大容量
    int cap;

    public LFUCache(int capacity) {
        keyToVal = new HashMap<>();
        keyToFreq = new HashMap<>();
        freqToKeys = new HashMap<>();
        this.cap = capacity;
        this.minFreq = 0;
    }

    public int get(int key) {}

    public void put(int key, int val) {}

}
```

LFU 的逻辑不难理解，但是写代码实现并不容易，因为你看我们要维护`KV`表，`KF`表，`FK`表三个映射，特别容易出错。注意事项：

1. 不要企图上来就实现算法的所有细节，而应该自顶向下，逐步求精，先写清楚主函数的逻辑框架，然后再一步步实现细节。
2. 搞清楚映射关系，如果我们更新了某个`key`对应的`freq`，那么就要同步修改`KF`表和`FK`表，这样才不会出问题。
3. 画图，画图，画图，重要的话说三遍，把逻辑比较复杂的部分用流程图画出来，然后根据图来写代码，可以极大减少出错的概率。

下面我们先来实现`get(key)`方法，逻辑很简单，返回`key`对应的`val`，然后增加`key`对应的`freq`：

```java
public int get(int key) {
    if (!keyToVal.containsKey(key)) {
        return -1;
    }
    // 增加 key 对应的 freq
    increaseFreq(key);
    return keyToVal.get(key);
}
```

增加`key`对应的`freq`是 LFU 算法的核心，所以我们干脆直接抽象成一个函数`increaseFreq`，这样`get`方法看起来就简洁清晰了。

下面来实现`put(key, val)`方法，逻辑略微复杂，我们直接画个图来看：

![图片](https://cos.duktig.cn/typora/202112022144504.webp)

```java
public void put(int key, int val) {
    if (this.cap <= 0) return;

    /* 若 key 已存在，修改对应的 val 即可 */
    if (keyToVal.containsKey(key)) {
        keyToVal.put(key, val);
        // key 对应的 freq 加一
        increaseFreq(key);
        return;
    }

    /* key 不存在，需要插入 */
    /* 容量已满的话需要淘汰一个 freq 最小的 key */
    if (this.cap <= keyToVal.size()) {
        removeMinFreqKey();
    }

    /* 插入 key 和 val，对应的 freq 为 1 */
    // 插入 KV 表
    keyToVal.put(key, val);
    // 插入 KF 表
    keyToFreq.put(key, 1);
    // 插入 FK 表
    freqToKeys.putIfAbsent(1, new LinkedHashSet<>());
    freqToKeys.get(1).add(key);
    // 插入新 key 后最小的 freq 肯定是 1
    this.minFreq = 1;
}
```

`increaseFreq`和`removeMinFreqKey`方法是 LFU 算法的核心，我们下面来看看怎么借助`KV`表，`KF`表，`FK`表这三个映射巧妙完成这两个函数。

**核心逻辑：**

首先来实现`removeMinFreqKey`函数：

```java
private void removeMinFreqKey() {
    // freq 最小的 key 列表
    LinkedHashSet<Integer> keyList = freqToKeys.get(this.minFreq);
    // 其中最先被插入的那个 key 就是该被淘汰的 key
    int deletedKey = keyList.iterator().next();
    /* 更新 FK 表 */
    keyList.remove(deletedKey);
    if (keyList.isEmpty()) {
        freqToKeys.remove(this.minFreq);
        // 问：这里需要更新 minFreq 的值吗？
    }
    /* 更新 KV 表 */
    keyToVal.remove(deletedKey);
    /* 更新 KF 表 */
    keyToFreq.remove(deletedKey);
}
```

删除某个键`key`肯定是要同时修改三个映射表的，借助`minFreq`参数可以从`FK`表中找到`freq`最小的`keyList`，根据时序，其中第一个元素就是要被淘汰的`deletedKey`，操作三个映射表删除这个`key`即可。

但是有个细节问题，如果`keyList`中只有一个元素，那么删除之后`minFreq`对应的`key`列表就为空了，也就是`minFreq`变量需要被更新。如何计算当前的`minFreq`是多少呢？

实际上没办法快速计算`minFreq`，只能线性遍历`FK`表或者`KF`表来计算，这样肯定不能保证 O(1) 的时间复杂度。

**但是，其实这里没必要更新`minFreq`变量**，因为你想想`removeMinFreqKey`这个函数是在什么时候调用？在`put`方法中插入新`key`时可能调用。而你回头看`put`的代码，插入新`key`时一定会把`minFreq`更新成 1，所以说即便这里`minFreq`变了，我们也不需要管它。

下面来实现`increaseFreq`函数：

```java
private void increaseFreq(int key) {
    int freq = keyToFreq.get(key);
    /* 更新 KF 表 */
    keyToFreq.put(key, freq + 1);
    /* 更新 FK 表 */
    // 将 key 从 freq 对应的列表中删除
    freqToKeys.get(freq).remove(key);
    // 将 key 加入 freq + 1 对应的列表中
    freqToKeys.putIfAbsent(freq + 1, new LinkedHashSet<>());
    freqToKeys.get(freq + 1).add(key);
    // 如果 freq 对应的列表空了，移除这个 freq
    if (freqToKeys.get(freq).isEmpty()) {
        freqToKeys.remove(freq);
        // 如果这个 freq 恰好是 minFreq，更新 minFreq
        if (freq == this.minFreq) {
            this.minFreq++;
        }
    }
}
```

更新某个`key`的`freq`肯定会涉及`FK`表和`KF`表，所以我们分别更新这两个表就行了。

和之前类似，当`FK`表中`freq`对应的列表被删空后，需要删除`FK`表中`freq`这个映射。如果这个`freq`恰好是`minFreq`，说明`minFreq`变量需要更新。

能不能快速找到当前的`minFreq`呢？这里是可以的，因为我们刚才把`key`的`freq`加了 1 嘛，所以`minFreq`也加 1 就行了。

至此，经过层层拆解，LFU 算法就完成了。

完整代码如下：

```java
public class LFUCache {

    /** key 到 val 的映射，我们后文称为 KV 表 */
    Map<Integer, Integer> keyToVal;

    /** key 到 freq 的映射，我们后文称为 KF 表 */
    Map<Integer, Integer> keyToFreq;

    /** freq 到 key 列表的映射，我们后文称为 FK 表 */
    Map<Integer, LinkedHashSet<Integer>> freqToKeys;

    /** 记录最小的频次 */
    int minFreq;

    /** 记录 LFU 缓存的最大容量 */
    int capacity;

    public LFUCache(int capacity) {
        keyToVal = new HashMap<>();
        keyToFreq = new HashMap<>();
        freqToKeys = new HashMap<>();
        this.capacity = capacity;
        this.minFreq = 0;
    }

    /**
     * 获取缓存值
     * 需要增加当前缓存的频次
     */
    public int get(int key) {
        if (! keyToVal.containsKey(key)) {
            return - 1;
        }
        // 增加 key 对应的 freq
        increaseFreq(key);
        return keyToVal.get(key);
    }

    /**
     * 新增或修改缓存值
     * 1. 若 key 已存在，修改对应的 val
     * 2. key 不存在，需要插入
     * 3. 容量已满的话需要淘汰一个 freq 最小的 key
     */
    public void put(int key, int val) {
        if (this.capacity <= 0) {
            return;
        }

        /* 若 key 已存在，修改对应的 val 即可 */
        if (keyToVal.containsKey(key)) {
            keyToVal.put(key, val);
            // key 对应的 freq 加一
            increaseFreq(key);
            return;
        }

        /* key 不存在，需要插入 */
        /* 容量已满的话需要淘汰一个 freq 最小的 key */
        if (this.capacity <= keyToVal.size()) {
            removeMinFreqKey();
        }

        /* 插入 key 和 val，对应的 freq 为 1 */
        // 插入 KV 表
        keyToVal.put(key, val);
        // 插入 KF 表
        keyToFreq.put(key, 1);
        // 插入 FK 表
        freqToKeys.putIfAbsent(1, new LinkedHashSet<>());
        freqToKeys.get(1).add(key);
        // 插入新 key 后最小的 freq 肯定是 1
        this.minFreq = 1;
    }

    /**
     * 删除最小频次，并且最久未使用的key
     * 即LinkedHashSet的头结点
     */
    private void removeMinFreqKey() {
        // freq 最小的 key 列表
        LinkedHashSet<Integer> keyList = freqToKeys.get(this.minFreq);
        // 其中最先被插入的那个 key 就是该被淘汰的 key
        int deletedKey = keyList.iterator().next();
        /* 更新 FK 表 */
        keyList.remove(deletedKey);
        if (keyList.isEmpty()) {
            freqToKeys.remove(this.minFreq);
            // 问：这里需要更新 minFreq 的值吗？
        }
        /* 更新 KV 表 */
        keyToVal.remove(deletedKey);
        /* 更新 KF 表 */
        keyToFreq.remove(deletedKey);
    }

    /**
     * 增加 key 对应的 freq
     */
    private void increaseFreq(int key) {
        int freq = keyToFreq.get(key);
        /* 更新 KF 表 */
        keyToFreq.put(key, freq + 1);
        /* 更新 FK 表 */
        // 将 key 从 freq 对应的列表中删除
        freqToKeys.get(freq).remove(key);
        // 将 key 加入 freq + 1 对应的列表中
        freqToKeys.putIfAbsent(freq + 1, new LinkedHashSet<>());
        freqToKeys.get(freq + 1).add(key);
        // 如果 freq 对应的列表空了，移除这个 freq
        if (freqToKeys.get(freq).isEmpty()) {
            freqToKeys.remove(freq);
            // 如果这个 freq 恰好是 minFreq，更新 minFreq
            if (freq == this.minFreq) {
                this.minFreq++;
            }
        }
    }
    
}
```

## 常数时间的增删改查

这个问题的⼀个技巧点在于，如何结合哈希表和数组，使得数组的删除操作时间复杂度也变成 O(1)？

### O(1) 时间插入、删除和获取随机元素

> #### [380. O(1) 时间插入、删除和获取随机元素](https://leetcode-cn.com/problems/insert-delete-getrandom-o1/)
>
> ```java
> class RandomizedSet {
> 
>  public RandomizedSet() {}
>  /** 如果 val 不存在集合中，则插⼊并返回 true，否则直接返回 false */ 
>  public boolean insert(int val) {}
>  /** 如果 val 在集合中，则删除并返回 true，否则直接返回 false */ 
>  public boolean remove(int val) {}
>  /** 从集合中等概率地随机获得⼀个元素 */ 
>  public int getRandom() {}
> 
> }
> ```

本题的难点在于两点：

1. **插⼊，删除，获取随机元素这三个操作的时间复杂度必须都是 O(1)。**
2. **`getRandom` ⽅法返回的元素必须等概率返回随机元素，也就是说，如果集合⾥⾯有 n 个元素，每个元素被返回的概率必须是 1/n**。

我们先来分析⼀下：对于插⼊，删除，查找这⼏个操作，哪种数据结构的时间复杂度是 O(1)？

`HashSet` 肯定算⼀个对吧。哈希集合的底层原理就是⼀个⼤数组，我们把元素通过哈希函数映射到⼀个索引上；如果⽤拉链法解决哈希冲突，那么这个索引可能连着⼀个链表或者红⿊树。那么请问对于这样⼀个标准的 `HashSet`，你能否在 O(1) 的时间内实现 `getRandom` 函数？

其实是不能的，因为根据刚才说到的底层实现，元素是被哈希函数「分散」到整个数组⾥⾯的，更别说还有拉链法等等解决哈希冲突的机制，基本做不到 O(1) 时间等概率随机获取元素。

`LinkedHashSet` 只是给 `HashSet` 增加了有序性，依然⽆法按要求实现我们的 `getRandom` 函数，因为底层⽤链表结构存储元素的话，是⽆法在 O(1) 的时间内访问某⼀个元素的。

根据上⾯的分析，对于 getRandom ⽅法，**如果想「等概率」且「在 O(1) 的时间」取出元素，⼀定要满⾜：底层⽤数组实现，且数组必须是紧凑的**。

这样我们就可以直接⽣成随机数作为索引，从数组中取出该随机索引对应的元素，作为随机元素。

**但如果⽤数组存储元素的话，插⼊，删除的时间复杂度怎么可能是 O(1) 呢？**

可以做到！对数组尾部进⾏插⼊和删除操作不会涉及数据搬移，时间复杂度是 O(1)。

**所以，如果我们想在 O(1) 的时间删除数组中的某⼀个元素 `val`，可以先把这个元素交换到数组的尾部，然后再 `pop` 掉**。

交换两个元素必须通过索引进⾏交换对吧，那么我们需要⼀个哈希表 `valToIndex` 来记录每个元素值对应的索引。

```java
public class RandomizedSet {

    List<Integer> nums;
    Map<Integer, Integer> valToIndex;
    Random rand = new Random();

    public RandomizedSet() {
        nums = new ArrayList<>();
        valToIndex = new HashMap<>();
    }

    /**
     * 新增元素
     *
     * @return 如果存在，返回false；不存在插入后，返回true
     */
    public boolean insert(int val) {
        if (valToIndex.containsKey(val)) {
            return false;
        }
        valToIndex.put(val, nums.size());
        nums.add(nums.size(), val);
        return true;
    }

    /**
     * 删除元素
     * 1. 数组将删除元素的下标，设置为最后一个元素的值
     * 2. 删除最后数组最后一个元素
     * 3. 维护map中元素和索引的映射关系
     */
    public boolean remove(int val) {
        // 若 val 不存在，不⽤再删除
        if (! valToIndex.containsKey(val)) {
            return false;
        }
        // 最后一个元素
        int lastElement = nums.get(nums.size() - 1);
        // 当前删除元素的索引
        int index = valToIndex.get(val);

        // 待删除元素索引设置为最后一个元素
        nums.set(index, lastElement);
        // 修改map中最后一个元素的索引指向
        valToIndex.put(lastElement, index);

        // list中删除最后一个元素
        nums.remove(nums.size() - 1);
        // map中删除待删除元素的索引映射
        valToIndex.remove(val);

        return true;
    }

    /**
     * @return 等概率返回其中一个元素
     */
    public int getRandom() {
        return nums.get(rand.nextInt(nums.size()));
    }

}
```

### 避开⿊名单的随机数

> **[710. 黑名单中的随机数](https://leetcode-cn.com/problems/random-pick-with-blacklist/)**
>
> 给定一个包含 [0，n) 中不重复整数的黑名单 blacklist ，写一个函数从 [0, n) 中返回一个不在 blacklist 中的随机整数。
>
> 对它进行优化使其尽量少调用系统方法 Math.random() 。
>
> ```java
> class Solution {
> 
>  public Solution(int n, int[] blacklist) {}
> 
>  /**在区间 [0,N) 中等概率随机选取⼀个元素并返回，这个元素不能是 blacklist 中的元素  */
>  public int pick() {}
> }
> ```

pick 函数会被多次调⽤，每次调⽤都要在区间 [0,N) 中「等概率随机」返回⼀个「不在 blacklist 中」的整数。

⽐如给你输⼊ N = 5, blacklist = [1,3]，那么多次调⽤ pick 函数，会等概率随机返回 0, 2, 4 中的某⼀个数字。

**⽽且题⽬要求，在 `pick` 函数中应该尽可能少调⽤随机数⽣成函数 `rand()`。**

**我们可以将区间 [0,N) 看做⼀个数组，然后将 blacklist 中的元素移到数组的最末尾，同时⽤⼀个哈希表进⾏映射**：

```java
public class BlackListRandom {

    /** 数组中黑明的分界线[sz,n)为黑名单的数 */
    int sz;
    /** 黑名单的映射 */
    Map<Integer, Integer> map;

    Random random = new Random();

    public BlackListRandom(int n, int[] blacklist) {
        map = new HashMap<>();
        sz = n - blacklist.length;
        // 初始化映射表
        for (int b : blacklist) {
            map.put(b, 0);
        }
        int last = n - 1;
        for (int b : blacklist) {
            // 如果 b 已经在区间 [sz, N) , 可以直接忽略
            if (b >= sz) {
                continue;
            }
            // 跳过无效索引
            while (map.containsKey(last)) {
                last--;
            }
            map.put(b, last);
            last--;
        }
    }

    public int pick() {
        // 随机选取⼀个索引
        int index = random.nextInt(sz);
        return map.getOrDefault(index, index);
    }

}
```

总结⼀下核⼼思想：
1、如果想⾼效地，等概率地随机获取元素，就要使⽤数组作为底层容器。

2、如果要保持数组元素的紧凑性，可以把待删除元素换到最后，然后 pop 掉末尾的元素，这样时间复杂度就是 O(1) 了。当然，我们需要额外的哈希表记录值到索引的映射。

3、对于第⼆题，数组中含有「空洞」（⿊名单数字），也可以利⽤哈希表巧妙处理映射关系，让数组在逻辑上是紧凑的，⽅便随机取元素。



### 数据流的中位数

> **[295. 数据流的中位数](https://leetcode-cn.com/problems/find-median-from-data-stream/)**
>
> 中位数是有序列表中间的数。如果列表长度是偶数，中位数则是中间两个数的平均值。
>
> [2,3,4] 的中位数是 3
>
> [2,3] 的中位数是 (2 + 3) / 2 = 2.5
>
> 设计一个支持以下两种操作的数据结构：
>
> `void addNum(int num)` - 从数据流中添加一个整数到数据结构中。
> `double findMedian()` - 返回目前所有元素的中位数。
>
> ```java
> class MedianFinder {
> 
>  public MedianFinder() {}
>  /** 添加⼀个数字 */
>  public void addNum(int num) {}
>  /** 计算当前添加的所有数字的中位数 */
>  public double findMedian() {}
> }
> 
> ```

一个直接的解法可以用一个数组记录所有`addNum`添加进来的数字，通过插入排序的逻辑保证数组中的元素有序，当调用`findMedian`方法时，可以通过数组索引直接计算中位数。

但是用数组作为底层容器的问题也很明显，`addNum`搜索插入位置的时候可以用二分搜索算法，但是插入操作需要搬移数据，所以最坏时间复杂度为 O(N)。

那换链表？链表插入元素很快，但是查找插入位置的时候只能线性遍历，最坏时间复杂度还是 O(N)，而且`findMedian`方法也需要遍历寻找中间索引，最坏时间复杂度也是 O(N)。

那么就用平衡二叉树呗，增删查改复杂度都是 O(logN)，这样总行了吧？

比如用 Java 提供的`TreeSet`容器，底层是红黑树，`addNum`直接插入，`findMedian`可以通过当前元素的个数推出计算中位数的元素的排名。

很遗憾，依然不行，这里有两个问题。

1. `TreeSet`是一种`Set`，其中不存在重复元素的元素，但是我们的数据流可能输入重复数据的，而且计算中位数也是需要算上重复元素的。
2. `TreeSet`并没有实现一个通过排名快速计算元素的 API。假设我想找到`TreeSet`中第 5 大的元素，并没有一个现成可用的方法实现这个需求。

除了平衡二叉树，还有没有什么常用的数据结构是动态有序的？优先级队列（二叉堆）行不行？

好像也不太行，因为优先级队列是一种受限的数据结构，只能从堆顶添加/删除元素，我们的`addNum`方法可以从堆顶插入元素，但是`findMedian`函数需要从数据中间取，这个功能优先级队列是没办法提供的。

**我们必然需要有序数据结构，本题的核心思路是使用两个优先级队列**。

中位数是有序数组最中间的元素算出来的对吧，我们可以把「有序数组」抽象成一个倒三角形，宽度可以视为元素的大小，那么这个倒三角的中部就是计算中位数的

然后我把这个大的倒三角形从正中间切成两半，变成一个小倒三角和一个梯形，这个小倒三角形相当于一个从小到大的有序数组，这个梯形相当于一个从大到小的有序数组。

中位数就可以通过小倒三角和梯形顶部的元素算出来对吧？嗯，你联想到什么了没有？它们能不能用优先级队列表示？**小倒三角不就是个大顶堆嘛，梯形不就是个小顶堆嘛，中位数可以通过它们的堆顶元素算出来**。

![image-20211203162439489](https://cos.duktig.cn/typora/202112031625330.png)

梯形虽然是小顶堆，但其中的元素是较大的，我们称其为`large`，倒三角虽然是大顶堆，但是其中元素较小，我们称其为`small`。

当然，这两个堆需要算法逻辑正确维护，才能保证堆顶元素是可以算出正确的中位数，**我们很容易看出来，两个堆中的元素之差不能超过 1**。

因为我们要求中位数嘛，假设元素总数是`n`，如果`n`是偶数，我们希望两个堆的元素个数是一样的，这样把两个堆的堆顶元素拿出来求个平均数就是中位数；如果`n`是奇数，那么我们希望两个堆的元素个数分别是`n/2 + 1`和`n/2`，这样元素多的那个堆的堆顶元素就是中位数。

**不仅要维护`large`和`small`的元素个数之差不超过 1，还要维护`large`堆的堆顶元素要大于等于`small`堆的堆顶元素**。

**想要往`large`里添加元素，不能直接添加，而是要先往`small`里添加，然后再把`small`的堆顶元素加到`large`中；向`small`中添加元素同理**。

```java
public class 数据流的中位数_295 {

    static class MedianFinder {

        PriorityQueue<Integer> large;
        PriorityQueue<Integer> small;

        public MedianFinder() {
            // 小顶堆
            large = new PriorityQueue<>();
            // 大顶堆
            small = new PriorityQueue<>((a, b) -> b - a);
        }

        public double findMedian() {
            // 如果元素不一样多，多的那个堆的堆顶元素就是中位数
            if (large.size() < small.size()) {
                return small.peek();
            } else if (large.size() > small.size()) {
                return large.peek();
            }
            // 如果元素一样多，两个堆堆顶元素的平均数是中位数
            return (large.peek() + small.peek()) / 2.0;
        }

        public void addNum(int num) {
            if (small.size() >= large.size()) {
                small.offer(num);
                large.offer(Objects.requireNonNull(small.poll()));
            } else {
                large.offer(num);
                small.offer(Objects.requireNonNull(large.poll()));
            }
        }
    }

}

```



