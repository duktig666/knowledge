# 大数据基础

## 大数据概念

> 大数据（Big Data）：指**无法在一定时间范围内**用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的**海量、高增长率和多样化**的**信息资产**。
>

按顺序给出数据存储单位：bit、Byte、KB、MB、GB、**TB、PB、EB**、ZB、YB、BB、NB、DB。

大数据主要解决，**海量数据的采集、存储和分析计算问题**。

## 大数据特点

1、Volume（大量）

截至目前，人类生产的所有印刷材料的数据量是200PB，而历史上全人类总共说过的话的数据量大约是5EB。当前，典型个人计算机硬盘的容量为TB量级，而**一些大企业的数据量已经接近EB量级**。

2、Velocity（高速）

这是大数据区分于传统数据挖掘的最显著特征。根据IDC的“数字宇宙”的报告，预计到2025年，全球数据使用量将达到163ZB。在如此海量的数据面前，处理数据的效率就是企业的生命。

天猫双十一：2017年3分01秒，天猫交易额超过100亿
2020年96秒，天猫交易额超过100亿

3、Variety（多样）

类型的多样性也让数据被分为结构化数据和非结构化数据。相对于以往便于存储的**以数据库/文本为主的结构化数据**，**非结构化**数据越来越多，包括**网络日志、音频、视频、图片、地理位置信息**等，这些多类型的数据对数据的处理能力提出了更高要求。

4、Value（低价值密度）

**价值密度的高低与数据总量的大小成反比。如何快速对有价值数据“提纯”成为目前大数据背景下待解决的难题。**

总结：

- **大数据量的存储**
- **高效的数据处理**
- **多样类型的数据**
- **快速提取有价值的信息**

## 大数据的应用场景

1、抖音：推荐的都是你喜欢的视频

2、电商站内广告推荐：给用户推荐可能喜欢的商品

3、零售：分析用户消费习惯，为用户购买商品提供方便，从而提升商品销量。用户常买的，有关联的商品放在一起（外国经典案例，纸尿布+啤酒）

4、物流仓储：京东物流，上午下单下午送达、下午下单次日上午送达。不同地区的仓库结合当地特点存储相应的物资

5、保险：海量数据挖掘及风险预测，助力保险行业精准营销，提升精细化定价能力。

6、金融：多维度体现用户特征，帮助金融机构推荐优质客户，防范欺诈风险。

7、房产：大数据全面助力房地产行业，打造精准投策与营销，选出更合适的地，建造更合适的楼，卖给更合适的人。

8、人工智能+ 5G + 物联网+ 虚拟与现实

## 大数据业务流程分析

![大数据业务流程分析](https://cos.duktig.cn/typora/202110061411924.png)

![大数据部门内组织结构](https://cos.duktig.cn/typora/202110061413554.png)

## 大数据技术生态体系

![大数据技术生态体系 ](https://cos.duktig.cn/typora/202110061414647.png)

1）Sqoop：Sqoop 是一款开源的工具，主要用于在 **Hadoop、Hive 与传统的数据库（MySQL）间进行数据的传递**，可以将一个关系型数据库（例如 ：MySQL，Oracle 等）中的数据导进到 Hadoop 的 HDFS 中，也可以将 HDFS 的数据导进到关系型数据库中。 

2）Flume：Flume 是一个高可用的，高可靠的，分布式的**海量日志采集、聚合和传输的系统**，Flume 支持在日志系统中定制各类数据发送方，用于收集数据； 

3）Kafka：Kafka 是一种高吞吐量的分布式发布订阅消息系统； 

4）Spark：Spark 是当前最流行的开源大数据内存计算框架。可以基于 Hadoop 上存储的大数据进行计算。 

5）Flink：Flink 是当前最流行的开源大数据内存计算框架。用于**实时计算**的场景较多。 

6）Oozie：Oozie 是一个管理 Hadoop 作业（job）的工作流程调度管理系统。 

7）Hbase：HBase 是一个**分布式的、面向列**的开源数据库。HBase 不同于一般的关系数据库，它是一个适合于**非结构化数据存储的数据库**。 

8）Hive：Hive 是基于 Hadoop 的一个**数据仓库工具**，可以将结构化的数据文件映射为一张数据库表，并提供简单的 SQL 查询功能，可以将 SQL 语句转换为 MapReduce 任务进行运行。其优点是学习成本低，可以通过类 SQL 语句快速实现简单的 MapReduce 统计，不必开发专门的MapReduce 应用，十分适合数据仓库的统计分析。 

9）ZooKeeper：它是一个针对大型分布式系统的**可靠协调系统**，提供的功能包括：配置维护、名字服务、分布式同步、组服务等。 

## 推荐系统项目框架

![image-20211024222223590](https://cos.duktig.cn/typora/202110242223328.png)

# 大数据技术梳理

## Hadoop

### HDFS

#### 什么是HDFS？

Hadoop Distributed File System，简称 **HDFS**，是一个**分布式文件系统**。 用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。 

> **分布式文件系统产生的背景**
>
> 随着数据量越来越大，在一个操作系统存不下所有的数据，那么就分配到更多的操作系统管理的磁盘中，但是不方便管理和维护，迫切**需要一种系统来管理多台机器上的文件**，这就是分布式文件管理系统。

#### HDFS的使用场景

**适合一次写入，多次读出的场景**。一个文件经过创建、写入和关闭之后就不需要改变。 

#### HDFS的优缺点

##### 优点

- **高容错性**
  - 数据自动保存多个副本。它通过增加副本的形式，提高容错性。
  - 某一个副本丢失以后，它可以自动恢复。
- **适合处理大数据**
  - 数据规模：能够处理数据规模达到GB、TB、甚至**PB级别的数据**；
  - 文件规模：能够处理百万规模以上的文件数量，数量相当之大。
- 可**构建在廉价机器上**，通过多副本机制，提高可靠性。

##### 缺点

- **不适合低延时数据访问**，比如毫秒级的存储数据，是做不到的。
- **无法高效的对大量小文件进行存储**
  - 存储大量小文件的话，它会占用NameNode大量的内存来存储文件目录和块信息。这样是不可取的，因为NameNode的内存总是有限的；
  - 小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标。
- **不支持并发写入、文件随机修改**
  - 一个文件只能有一个写，不允许多个线程同时写
  - 仅支持数据append（追加），不支持文件的随机修改

### MapReduce

#### 什么是MapReduce？

MapReduce 是一个**分布式运算程序**的编程框架，是用户开发“基于 Hadoop 的数据分析应用”的核心框架。 

MapReduce 核心功能是 **将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个 Hadoop 集群上** 。 

#### MapReduce 的使用场景

核心思想：**如何把大问题分解成独立的小问题，再并行解决**。

典型场景：

**计算`URL`的访问频率**：搜索引擎的使用中，会遇到大量的`URL`的访问，所以，可以使用 `MapReduce` 来进行统计，得出（`URL`,次数）结果，在后续的分析中可以使用。

**Top K 问题**：在各种的文档分析，或者是不同的场景中，经常会遇到关于 `Top K` 的问题，例如输出这篇文章的出现前`5`个最多的词汇。这个时候也可以使用 `MapReduce`来进行统计。

#### MapReduce优缺点

##### 优点

1、**易于编程**： 用户只关心业务逻辑。 实现框架的接口。
2、**良好扩展性**：可以动态增加服务器，解决计算资源不够问题。
3、**高容错性**：任何一台机器挂掉，可以将任务转移到其他节点。
4、**适合海量数据计算**：（TB/PB） 几千台服务器共同计算。

##### 缺点

1、**不擅长实时计算。 Mysql**（在毫秒或者秒级内返回结果）
2、**不擅长流式计算。 Spark Streaming | flink 。**流式计算的输入数据是动态的，而 MapReduce 的输入数据集是静态的，不能动态变化。
3、**不擅长DAG有向无环图计算。spark 。** 多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce 并不是不能做，而是使用后，每个 MapReduce 作业的输出结果都会写入到磁盘，会造成大量的磁盘 IO，导致性能非常的低下。 

### Yarn

#### 什么是Yarn？

**Yarn**是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的**操作系统平台**，而 **MapReduce** 等运算程序则相当于**运行于操作系统之上的应用程序**。

#### Yarn主要解决的问题

- 如何管理集群资源？
- 如何给任务合理分配资源？



## Hive

### 什么是 Hive ？

#### Hive简介

Hive：由Facebook 开源用于解决 **海量结构化日志的数据统计** 的工具。 

Hive 是基于 Hadoop 的一个 **数据仓库工具**，可以 **将结构化的数据文件映射为一张表，并提供类 SQL 查询功能**。 

#### Hive本质

**将 HQL 转化成MapReduce 程序** 。主要如下：

- Hive 处理的数据存储在 HDFS 
- Hive 分析数据底层的实现是 MapReduce 
- 执行程序运行在 Yarn 上 

### Hive 的优缺点

#### 优点

- 操作接口采用 **类 SQL 语法**，提供快速开发的能力（简单、容易上手）。 
- **避免了去写 MapReduce**，减少开发人员的学习成本。 
- Hive可以处理大数据量。
- Hive 支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。 

#### 缺点

- Hive 的 HQL 表达能力有限 
  - 迭代式算法无法表达 
  - 数据挖掘方面不擅长，由于 MapReduce 数据处理流程的限制，效率更高的算法却无法实现。 
- Hive 的效率比较低 
  - Hive 自动生成的 MapReduce 作业，通常情况下不够智能化 
  - Hive 调优比较困难，粒度较粗 
- Hive的执行延迟比较高。

### Hive 的使用场景

因为Hive 的执行延迟比较高，所以

1. Hive 常用于数据分析，对实时性要求不高的场合。
2. Hive 优势在于处理大数据，对于处理小数据没有优势。 

## HBase

### 什么是 Hbase？

HBase是一种分布式、可扩展、支持海量数据存储的 **NoSQL数据库**。 

HBase是依赖Hadoop的。为什么HBase能存储海量的数据？**因为HBase是在HDFS的基础之上构建的，HDFS是分布式文件系统**。

HBase在HDFS之上提供了**高并发的随机写和支持实时查询**，这是HDFS不具备的。

### HBase的优缺点

优点：

1. **低成本**来**存储海量**的数据并且支持高并发随机写和实时查询
2. 基于「列式存储」，**存储数据的”结构“可以地非常灵活**



参看：

- [我终于看懂了HBase，太不容易了...](https://zhuanlan.zhihu.com/p/145551967)
- [一文读懂 HBase 使用场景](https://blog.csdn.net/u011598442/article/details/89891926)



## Sqoop

### 什么是Sqoop？

Sqoop 是一款开源的工具，主要用于在 **Hadoop(Hive)与传统的数据库(mysql、postgresql...) 间进行数据的传递**，可以将一个关系型数据库（例如 ： MySQL ,Oracle ,Postgres 等）中的数据导进到 Hadoop 的 HDFS 中，也可以将 HDFS 的数据导进到关系型数据库中。

### Sqoop原理

将导入或导出命令翻译成 mapreduce 程序来实现，在翻译出的 mapreduce 中主要是对 `inputformat` 和 `outputformat` 进行定制。



## Flume

### 什么是Flume？

Flume 是Cloudera 提供的一个高可用的，高可靠的，**分布式的海量日志采集、聚合和传输的系统**。Flume 基于流式架构，灵活简单。 

Flume最主要的作用就是，**实时读取服务器本地磁盘的数据，将数据写入到HDFS**。

![Flume作用](https://cos.duktig.cn/typora/202110251052295.png)

## Spark

### 什么是Spark？

Spark 是一种由Scala 语言开发，基于内存的快速、通用、可扩展的大数据分析计算引擎。 

### Spark和Hadoop的关系

Spark 和Hadoop 的根本差异是多个作业之间的数据通信问题 : Spark 多个作业之间数据通信是基于内存，而 Hadoop 是基于磁盘。

### Spark的核心模块

#### Spark Core

Spark Core 中提供了 Spark 最基础与最核心的功能，Spark 其他的功能如：Spark SQL，Spark Streaming，GraphX, MLlib 都是在Spark Core 的基础上进行扩展的 

#### Spark SQL

Spark SQL 是Spark 用来操作结构化数据的组件。通过Spark SQL，用户可以使用 SQL或者Apache Hive 版本的 SQL 方言（HQL）来查询数据。 

#### Spark Streaming

Spark Streaming 是 Spark 平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的API。 

#### Spark MLlib

MLlib 是 Spark 提供的一个机器学习算法库。MLlib 不仅提供了模型评估、数据导入等额外的功能，还提供了一些更底层的机器学习原语。

#### Spark GraphX

GraphX 是Spark 面向图计算提供的框架与算法库。 

参看：

- [Spark——需要HADOOP来运行SPARK吗？](https://www.cnblogs.com/caoweixiong/p/13426876.html)

##  Flink

### 什么是Flink？

Apache Flink 是一个**框架**和**分布式处理引擎**，用于**对无界和有界数据流进行状态计算**。

### 为什么选择 Flink？

流数据更真实地反映了我们的生活方式

传统的数据架构是基于有限数据集的

优点：

- 低延迟
- 高吞吐
- 结果的准确性和良好的容错性

### 流数据使用场景

电商和市场营销：数据报表、广告投放、业务流程需要

物联网（IOT）：传感器实时数据采集和显示、实时报警，交通运输业

电信业：基站流量调配

银行和金融业：实时结算和通知推送，实时检测异常行为

### Flink 的主要特点

1. 事件驱动（Event-driven）
2. 基于流的世界观
   1. 在 Flink 的世界观中，一切都是由流组成的，离线数据是有界的流；
   2. 实时数据是一个没有界限的流：这就是所谓的有界流和无界流
3. 分层API
   1. 越顶层越抽象，表达含义越简明，使用越方便
   2. 越底层越具体，表达能力越丰富，使用越灵活

其他特点：

1. 支持事件时间（event-time）和处理时间（processing-time）语义
2. 精确一次（exactly-once）的状态一致性保证
3. 低延迟，每秒处理数百万个事件，毫秒级延迟
4. 与众多常用存储系统的连接
5. 高可用，动态扩展，实现7*24小时全天候运行

### Flink vs Spark Streaming

**数据模型**

- spark 采用 RDD 模型，spark streaming 的 DStream 实际上也就是一组 组小批数据 RDD 的集合
- flink 基本数据模型是数据流，以及事件（Event）序列

**运行时架构**

- spark 是批计算，将 DAG 划分为不同的 stage，一个完成后才可以计算下一个
- flink 是标准的流执行模式，一个事件在一个节点处理完后可以直接发往下一个节点进行处理

## Mahout（过时淘汰）

### Mahout是什么？

Mahout是一个**算法库**，提供一些可扩展的**机器学习领域经典算法的实现**，旨在帮助开发人员更加方便快捷地创建智能应用程序。

Mahout包含许多实现，包括聚类、分类、推荐过滤、频繁子项挖掘。



可以被 Spark Mlib替换

[Mahout介绍和简单应用](https://www.cnblogs.com/ahu-lichang/p/7073836.html)



## Impala

### 什么是Impala？

Impala是Cloudera由C++编写的基于MPP（massively parallel processing）理念的查询引擎，由运行在CDH集群上的不同的守护进程组成，它跟Hive的metastore集成，共用database和tables等信息。

### **优势**

- impala跟现有的CDH组件自动集成，数据可以被CDH中的各种组件共用
- 支持sql查询hbase、hdfs、kudu等。
- impala只需要几秒钟或者分钟级别就能返回数据
- 支持parquet、text、rcfile、hfile等文件格式



## KUDU

### 什么是Kudu？

KUDU 的定位是 「Fast Analytics on Fast Data」，是一个既支持随机读写、又支持 OLAP 分析的大数据存储引擎。

KUDU 是一个「折中」的产品，在 HDFS 和 HBase 这两个偏科生中平衡了随机读写和批量分析的性能。

### 为什么要使用Kudu？

在 KUDU 之前，大数据主要以两种方式存储：

- **静态数据**：以 HDFS 引擎作为存储引擎，适用于高吞吐量的离线大数据分析场景。这类存储的局限性是数据无法进行随机的读写。
- **动态数据**：以 HBase、Cassandra 作为存储引擎，适用于大数据随机读写场景。这类存储的局限性是批量读取吞吐量远不如 HDFS，不适用于批量数据分析的场景。

从上面分析可知，这两种数据在存储方式上完全不同，进而导致使用场景完全不同，但在真实的场景中，边界可能没有那么清晰，面对既需要随机读写，又需要批量分析的大数据场景，该如何选择呢？

[KUDU 介绍](https://www.jianshu.com/p/93c602b637a4)







